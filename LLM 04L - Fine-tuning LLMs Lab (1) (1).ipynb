{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de16584d-aaf8-48f8-8e75-7bef1a7c6fdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb490bad-6576-468a-b295-cd8dadb14c13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 04L - Fine-tuning LLMs\n",
    "In this lab, we will apply the fine-tuning learnings from the demo Notebook. The aim of this lab is to fine-tune an instruction-following LLM.\n",
    "\n",
    "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "1. Prepare a novel dataset\n",
    "1. Fine-tune the T5-small model to classify movie reviews.\n",
    "1. Leverage DeepSpeed to enhance training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "497107f0-7f16-49c9-8dd9-f52db07f024e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert \"gpu\" in spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\"), \"THIS LAB REQUIRES THAT A GPU MACHINE AND RUNTIME IS UTILIZED.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9c341e9-392a-47e3-a109-bd9d300c74fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23f8ef92-c7f3-4f34-86fa-71eef8b31ee7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting rouge_score==0.1.2\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nRequirement already satisfied: absl-py in /databricks/python3/lib/python3.10/site-packages (from rouge_score==0.1.2) (1.0.0)\nRequirement already satisfied: nltk in /databricks/python3/lib/python3.10/site-packages (from rouge_score==0.1.2) (3.7)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.10/site-packages (from rouge_score==0.1.2) (1.21.5)\nRequirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge_score==0.1.2) (1.16.0)\nRequirement already satisfied: click in /databricks/python3/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (8.0.4)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (4.64.1)\nRequirement already satisfied: regex>=2021.8.3 in /databricks/python3/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (2022.7.9)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (1.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py): started\n  Building wheel for rouge_score (setup.py): finished with status 'done'\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=fdedeed32e09d331a62bb8cafda4bf93ca67ff5b769e8ff6167dea0bf06d69a1\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install rouge_score==0.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e784330-0890-4df4-b926-14ac74f0dfcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| enumerating serving endpoints...found 0...(0 seconds)\n| No action taken\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/large-language-models/v01\"\n\nValidating the locally installed datasets:\n| listing local files...(7 seconds)\n| removing extra path: /models--t5-base/blobs/...(0 seconds)\n| removing extra path: /models--t5-small/.no_exist/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/...(0 seconds)\n| removing extra path: /models--t5-small/blobs/...(1 seconds)\n| removing extra path: /models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/...(0 seconds)\n| fixed 4 issues...(8 seconds total)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing lab testing framework.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nUsing the \"default\" schema.\n\nPredefined paths variables:\n| DA.paths.working_dir: /dbfs/mnt/dbacademy-users/labuser4466139@vocareum.com/large-language-models\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/labuser4466139@vocareum.com/large-language-models/database.db\n| DA.paths.datasets:    /dbfs/mnt/dbacademy-datasets/large-language-models/v01\n\nSetup completed (17 seconds)\n\nThe models developed or used in this course are for demonstration and learning purposes only.\nModels may occasionally output offensive, inaccurate, biased information, or harmful instructions.\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a25cf2e-e28c-4cc6-abb3-795161ba0a0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username:          labuser4466139@vocareum.com\nWorking Directory: /dbfs/mnt/dbacademy-users/labuser4466139@vocareum.com/large-language-models\n"
     ]
    }
   ],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9619e31-7190-4b86-a564-8ce8f5184668",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1e2d895-585c-415f-92d3-f41cdc4dde41",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Creating a local temporary directory on the Driver. This will serve as a root directory for the intermediate model checkpoints created during the training process. The final model will be persisted to DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c87cc7b1-ca98-4ed4-a4d3-0bd18d923b47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "tmpdir = tempfile.TemporaryDirectory()\n",
    "local_training_root = tmpdir.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1228d0d1-5cfa-463e-ae51-0c93e1cbe1c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "427b5c55-5bb2-4ee0-86bd-bdf414ac3ea1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    Trainer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f492d29-af6c-4e53-9d71-f484b4b1102d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 1: Data Preparation\n",
    "For the instruction-following use cases we need a dataset that consists of prompt/response pairs along with any contextual information that can be used as input when training the model. The [databricks/databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) is one such dataset that provides high-quality, human-generated prompt/response pairs. \n",
    "\n",
    "Let's start by loading this dataset using the `load_dataset` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e1bae8-9161-4576-8ba8-d26963332b28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:26: UserWarning: This dataset can not be stored in DBFS because either `cache_dir` or the environment variable `HF_DATASETS_CACHE` is set to a non-DBFS path. If this cluster restarts, all saved dataset information will be lost.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c853072a614c1690a7430b3034abe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:13: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/databricks--databricks-dolly-15k to /root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d853cff557bd4e78bf66c48ca84723df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2a4673c91e44c680f667628addd203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1ca69a86e44277943db65f423d2784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04865bbc1734a418bec813fc05d54a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86eb189d94d54de595d3fb43e874777f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "ds = load_dataset(\"databricks/databricks-dolly-15k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77784709-904f-4071-a16e-dbcd706c7fa4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'context', 'response', 'category'],\n        num_rows: 15011\n    })\n})\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79e7b3b1-4a49-4b0e-a58e-eeec437f7138",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson4, question1\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion4_1(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2527d0bb-5b3c-44ad-8187-640ca60102bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 2: Select pre-trained model\n",
    "\n",
    "The model that we are going to fine-tune is [pythia-70m-deduped](https://huggingface.co/EleutherAI/pythia-70m-deduped). This model is one of a Pythia Suite of models that have been developed to support interpretability research.\n",
    "\n",
    "Let's define the pre-trained model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9272c709-0a9a-4fd4-a5b7-cf096a0a393d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "model_checkpoint = \"EleutherAI/pythia-70m-deduped\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75c76f94-6559-4889-aa08-81d293ce434a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/pythia-70m-deduped\n"
     ]
    }
   ],
   "source": [
    "print(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc3e21c6-2ad3-4e88-ac1c-dd191358dca8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson4, question2\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion4_2(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b4f67a1-b85b-4dc1-87cb-118570ad070a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 3: Load and Configure\n",
    "\n",
    "The next task is to load and configure the tokenizer for this model. The instruction-following process builds a body of text that contains the instruction, context input, and response values from the dataset. The body of text also includes some special tokens to identify the sections of the text. These tokens are generally configurable, and need to be added to the tokenizer.\n",
    "\n",
    "Let's go ahead and load the tokenizer for the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f3b14c-59d2-43c1-abce-0632eef6fd9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1240cf4a1ada4cc68c7a5fcfc75bede0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/huggingface_hub/file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in /dbfs/mnt/dbacademy-datasets/large-language-models/v01. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n  warnings.warn(message)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ceb2555c1634dd9ae0000fd8839d709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75df963a08d94988af885ecea917826e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "# load the tokenizer that was used for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  model_checkpoint,cache_dir=DA.paths.datasets\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": [\"### End\", \"### Instruction:\", \"### Response:\\n\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93dcd302-9271-4b1f-a976-c69ac107b168",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXTokenizerFast(name_or_path='EleutherAI/pythia-70m-deduped', vocab_size=50254, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['### End', '### Instruction:', '### Response:\\n']}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2157b675-e39c-4f74-aabe-1499d5133dec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson4, question3\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion4_3(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df59430e-f535-465e-b404-df3c7d932649",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 4: Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c123756a-8af2-47d2-86d1-95d221ee001d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The `tokenize` method below builds the body of text for each prompt/response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b71655e-5a48-4435-b75e-aa1cfe564a13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "remove_columns = [\"instruction\", \"response\", \"context\", \"category\"]\n",
    "\n",
    "\n",
    "def tokenize(x: dict, max_length: int = 1024) -> dict:\n",
    "    \"\"\"\n",
    "    For a dictionary example of instruction, response, and context a dictionary of input_id and attention mask is returned\n",
    "    \"\"\"\n",
    "    instr = x[\"instruction\"]\n",
    "    resp = x[\"response\"]\n",
    "    context = x[\"context\"]\n",
    "\n",
    "    instr_part = f\"### Instruction:\\n{instr}\"\n",
    "    context_part = \"\"\n",
    "    if context:\n",
    "        context_part = f\"\\nInput:\\n{context}\\n\"\n",
    "    resp_part = f\"### Response:\\n{resp}\"\n",
    "\n",
    "    text = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "{instr_part}\n",
    "{context_part}\n",
    "{resp_part}\n",
    "\n",
    "### End\n",
    "\"\"\"\n",
    "    return tokenizer(text, max_length=max_length, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd8de14c-6538-4104-a90d-004a717ac9a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's `tokenize` the Dolly training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ada2e7b8-7887-4791-a8ba-1ec9eebb6395",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438a9763b8614da59627d7794404d22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "tokenized_dataset = ds.map(tokenize,batched=True,remove_columns=remove_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf48d34-92fd-4492-bcc2-2b5d76af84d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 16384\n    })\n})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed723127-7f42-47b7-869d-60a0bfd37a45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30003, 310, 271, 9775, 326, 8631, 247, 4836, 15, 19566, 247, 2380, 326, 20420, 29141, 253, 2748, 15, 535, 50278, 187, 5013, 3039, 858, 8237, 6976, 1265, 6498, 32, 1383, 686, 7371, 310, 247, 3417, 273, 6773, 32, 308, 1714, 390, 416, 1714, 1383, 686, 4967, 476, 4049, 1241, 11883, 323, 1048, 1293, 1824, 32, 1383, 346, 2422, 547, 434, 4651, 452, 1264, 19367, 27, 22138, 13, 18827, 90, 13, 285, 752, 457, 84, 253, 1416, 273, 253, 2626, 6122, 46607, 686, 3039, 369, 6270, 80, 11938, 17644, 263, 4355, 5686, 32, 1383, 686, 2042, 309, 452, 625, 7437, 387, 253, 673, 273, 32481, 358, 366, 13, 452, 309, 1912, 32, 1383, 686, 15768, 247, 3806, 2505, 670, 418, 2555, 522, 267, 3288, 4019, 13, 835, 1057, 352, 1379, 1659, 13, 665, 3053, 352, 285, 752, 310, 352, 32, 1383, 686, 7883, 3534, 253, 7379, 253, 2659, 275, 10310, 281, 1973, 616, 41053, 1383, 686, 4967, 6109, 310, 3076, 323, 1966, 1383, 686, 7883, 369, 2516, 25084, 19896, 920, 32, 1383, 686, 7883, 310, 7195, 19759, 32, 1383, 686, 7883, 369, 25856, 10049, 1503, 1190, 4882, 1411, 672, 344, 11691, 5540, 273, 521, 66, 6671, 9901, 2792, 32, 1383, 346, 4509, 253, 10056, 1618, 1066, 253, 3672, 323, 534, 11128, 1578, 6470, 66, 312, 310, 45914, 434, 954, 11906, 2846, 15, 5552, 253, 1543, 275, 39169, 9070, 5981, 17295, 686, 1276, 310, 247, 38019, 32, 1383, 686, 2347, 513, 309, 1265, 3515, 32, 1383, 686, 7371, 13305, 273, 2952, 1740, 273, 10850, 273, 596, 22334, 858, 28331, 5602, 45, 6950, 1480, 32, 1383, 686, 1276, 310, 1232, 15067, 32, 1383, 686, 1276, 403, 690, 4451, 31261, 13898, 22513, 326, 368, 476, 1056, 4834, 32, 1383, 686, 1276, 310, 247, 15218, 422, 49615, 32, 1383, 686, 11540, 1419, 534, 7935, 310, 2876, 390, 49298, 27, 21860, 15354, 13, 443, 438, 536, 1383, 686, 19735, 479, 253, 1755, 608, 15692, 6500, 2567, 4454, 40219, 686, 1276, 2060, 556, 1912, 253, 954, 19898, 5328, 40615, 275, 253, 2892, 273, 253, 3958, 32, 1383, 686, 7371, 13986, 9558, 16264, 773, 38186, 342, 247, 29169, 40627, 804, 43820, 1383, 686, 1276, 6569, 672, 253, 5101, 4566, 1066, 32, 1383, 686, 1276, 310, 247, 17257, 32, 1383, 686, 7992, 974, 253, 7302, 273, 25852, 2061, 73, 5391, 285, 247, 16485, 273, 253, 1027, 3510, 273, 15908, 317, 266, 8458, 326, 25852, 2061, 73, 5391, 556, 4197, 323, 697, 31449, 631, 9025, 40219, 686, 7371, 273, 253, 1563, 310, 4217, 323, 13120, 27, 247, 5253, 273, 8995, 13, 247, 18067, 13, 247, 6194, 13, 271, 209, 5491, 23636, 23361, 13, 247, 6415, 13, 247, 25812, 13, 271, 19126, 285, 247, 660, 37449, 40219, 686, 11540, 1419, 534, 7935, 310, 2876, 390, 5534, 16668, 27, 29347, 11317, 13, 1503, 662, 66, 1383, 686, 1276, 310, 253, 3388, 36846, 273, 247, 15790, 6724, 363, 972, 32, 1383, 686, 7883, 3395, 6963, 273, 23253, 275, 1283, 3071, 32, 1383, 686, 1276, 310, 1682, 10223, 20502, 390, 26252, 1383, 686, 7883, 4546, 19669, 253, 26184, 275, 380, 15584, 10535, 264, 22222, 1383, 686, 22309, 858, 16739, 39457, 1669, 253, 27653, 41931, 32, 1383, 686, 5371, 310, 30392, 8990, 1383, 686, 1276, 310, 762, 17695, 32, 1383, 686, 19735, 479, 247, 1618, 273, 690, 5319, 273, 247, 1175, 8574, 8979, 32, 1383, 686, 7845, 26799, 752, 29743, 249, 1057, 40219, 686, 7883, 9809, 1244, 409, 15055, 432, 253, 6150, 19827, 1383, 686, 2347, 476, 309, 8162, 281, 33308, 32, 1383, 686, 7883, 403, 29792, 4479, 510, 418, 27198, 26444, 3, 3736, 1383, 686, 11888, 6667, 2668, 432, 253, 2505, 1918, 479, 247, 6010, 273, 253, 2022, 7125, 275, 9796, 273, 21245, 294, 1148, 569, 275, 253, 1986, 2077, 285, 253, 17683, 2105, 273, 14937, 272, 824, 294, 1148, 569, 1383, 686, 3039, 369, 253, 9530, 23179, 32, 1383, 686, 6343, 247, 6200, 875, 767, 14142, 16585, 6440, 496, 2409, 1383, 686, 8917, 13223, 43329, 452, 11233, 32, 1383, 686, 6723, 298, 5348, 4999, 323, 16581, 32, 1383, 686, 1276, 310, 4146, 25037, 1720, 22979, 32, 1383, 686, 1276, 310, 253, 14731, 275, 897, 275, 253, 16333, 32, 1383, 686, 1276, 310, 253, 1682, 23055, 2962, 275, 253, 1533, 1383, 686, 7883, 369, 253, 806, 281, 1924, 253, 20389, 868, 285, 367, 968, 4498, 32, 1383, 686, 11509, 504, 479, 247, 1618, 273, 253, 1027, 3510, 273, 26880, 265, 908, 275, 5561, 21266, 1383, 686, 26068, 273, 690, 2021, 4803, 281, 8591, 247, 5875, 2021, 2954, 1383, 686, 688, 253, 2962, 329, 16865, 273, 22078, 285, 8726, 13, 665, 310, 253, 15217, 273, 3995, 12604, 296, 782, 32, 1383, 686, 1276, 310, 18839, 2329, 285, 2139, 651, 7940, 25760, 342, 18839, 2329, 32, 1383, 686, 1276, 3085, 1912, 253, 23430, 24962, 347, 1682, 3085, 1383, 686, 4609, 40581, 625, 13, 5412, 390, 3511, 1824, 32, 1383, 686, 15768, 436, 12494, 13, 752, 310, 253, 1755, 3885, 273, 247, 611, 571, 659, 4940, 32, 1383, 686, 10639, 247, 2159, 12494, 670, 2139, 368, 943, 417, 452, 1097, 247, 7590, 5798, 285, 247, 7590, 12621, 40219, 686, 1276, 310, 634, 7583, 11705, 432, 253, 17478, 36858, 285, 24756, 12133, 32, 1383, 686, 1276, 2789, 247, 7212, 581, 1113, 594, 3809, 32, 1383, 686, 20696, 32249, 3587, 432, 253, 2505, 1918, 479, 247, 6010, 273, 253, 2892, 273, 253, 10030, 418, 553, 27695, 40219, 686, 1276, 310, 48869, 51, 32, 1383, 686, 2513, 10763, 8103, 390, 17854, 32, 1383, 686, 11540, 1419, 534, 7935, 310, 2876, 390, 49298, 27, 611, 4029, 29402, 13, 41964, 11889, 1383, 686, 4967, 513, 14408, 11291, 15284, 2489, 46745, 846, 5909, 32, 1383, 686, 4947, 1419, 1016, 273, 253, 1563, 347, 247, 3625, 3295, 390, 247, 6561, 3295, 1383, 686, 7161, 403, 253, 4782, 8237, 18708, 313, 13152, 42, 10, 285, 534, 17546, 403, 247, 629, 273, 253, 378, 15778, 32, 1383, 686, 10795, 346, 483, 3381, 7022, 3, 390, 346, 249, 3381, 7022, 3, 5649, 253, 22167, 625, 32, 1383, 686, 1276, 310, 253, 2852, 323, 1966, 32, 1383, 686, 2402, 690, 273, 253, 45980, 1306, 34174, 432, 40256, 81, 1765, 16228, 1383, 686, 2402, 690, 8530, 5561, 10604, 432, 253, 11994, 84, 1383, 30003, 310, 271, 9775, 326, 8631, 247, 4836, 15, 19566, 247, 2380, 326, 20420, 29141, 253, 2748, 15, 535, 50278, 187, 5013, 1276, 5997, 247, 29276, 32, 1383, 686, 3039, 403, 33387, 954, 1846, 275, 253, 3729, 5396, 31567, 32, 1383, 686, 1276, 310, 253, 1682, 4328, 323, 247, 2021, 18125, 275, 10439, 3060, 32, 1383, 346, 1276, 369, 2398, 4218, 434, 10301, 46607, 686, 11509, 504, 247, 1618, 273, 4088, 20230, 476, 6233, 6021, 432, 616, 3440, 11072, 40219, 686, 1276, 310, 253, 3872, 1388, 273, 6176, 32, 1383, 686, 1276, 310, 4194, 272, 32, 1383, 346, 15768, 436, 12494, 670, 7531, 13, 2139, 7531, 434, 591, 37297, 50276, 40, 9308, 310, 1698, 46607, 686, 15768, 253, 7418, 16620, 275, 11432, 6884, 27141, 13, 752, 310, 253, 38916, 353, 698, 3645, 12234, 5595, 4009, 8584, 20929, 313, 46, 18355, 10, 11240, 32, 1383, 686, 1276, 403, 253, 1604, 255, 1131, 285, 557, 24301, 255, 1131, 273, 49681, 342, 31739, 10941, 281, 8458, 32, 1383, 686, 15768, 247, 3806, 2505, 670, 743, 24318, 427, 1920, 14059, 267, 13, 752, 858, 521, 789, 2770, 327, 32, 1383, 686, 1276, 310, 247, 6867, 388, 1765, 2135, 1536, 1079, 10346, 3102, 1383, 686, 11509, 504, 247, 1618, 273, 2133, 4243, 326, 403, 760, 1264, 4876, 1048, 15, 3856, 403, 690, 2133, 4243, 326, 403, 1264, 1339, 3606, 3000, 27, 25283, 13, 9412, 13, 5130, 13, 2472, 13, 1791, 285, 21065, 40219, 686, 7883, 310, 253, 2022, 5248, 275, 1023, 2050, 10391, 5579, 921, 32, 1383, 686, 510, 2929, 346, 6601, 1075, 310, 512, 368, 878, 3, 4081, 253, 4480, 19946, 1566, 323, 253, 806, 673, 13, 534, 574, 247, 15585, 3486, 327, 253, 6774, 21708, 46, 15, 7764, 1918, 253, 2022, 5161, 2934, 273, 4480, 19946, 2529, 275, 436, 2929, 1383, 686, 4947, 1419, 841, 15737, 1754, 327, 616, 3295, 4880, 10602, 2345, 4461, 13, 42154, 13, 7318, 49438, 12973, 13, 1795, 43363, 1383, 686, 2347, 588, 12939, 40, 5736, 285, 643, 1781, 3448, 3210, 1055, 4730, 285, 11369, 32870, 32, 1383, 686, 1276, 310, 253, 2629, 36641, 1959, 2341, 1383, 686, 5371, 310, 18598, 9145, 1383, 686, 2347, 1142, 2069, 369, 25990, 18732, 6290, 466, 2400, 275, 697, 1048, 2892, 32, 1383, 686, 1276, 310, 253, 11216, 383, 5893, 327, 7565, 32, 1383, 686, 19735, 479, 247, 1618, 273, 4633, 3668, 21110, 281, 8028, 281, 1383, 686, 2347, 943, 368, 1557, 323, 41417, 32, 1383, 686, 11778, 368, 3735, 273, 399, 3841, 13816, 263, 12828, 796, 6156, 16511, 32, 1383, 686, 7845, 30215, 432, 534, 33010, 14, 11125, 10325, 403, 1563, 11321, 13, 390, 23055, 2962, 27, 380, 1091, 273, 253, 44642, 13, 18147, 11122, 898, 13, 48255, 1383, 346, 4476, 19268, 247, 36108, 7156, 368, 1849, 574, 17295, 686, 2513, 4715, 18542, 327, 247, 15487, 816, 347, 3576, 347, 4715, 327, 247, 4936, 16, 484, 918, 18542, 32, 1383, 686, 36447, 826, 37281, 13, 44065, 8741, 2153, 13, 418, 3230, 20757, 752, 275, 1846, 1383, 686, 7992, 974, 512, 273, 253, 12282, 5393, 275, 436, 12494, 285, 1618, 731, 970, 29093, 275, 253, 5981, 551, 6958, 94, 428, 551, 11185, 94, 1383, 686, 4967, 513, 2151, 273, 5396, 38356, 513, 973, 275, 2101, 437, 35, 1796, 39858, 275, 253, 5106, 32, 1383, 686, 19735, 479, 247, 1643, 4088, 326, 309, 476, 755, 10046, 285, 275, 1805, 5281, 1078, 253, 1735, 21114, 2952, 40219, 686, 17570, 479, 1880, 841, 403, 5579, 2722, 390, 11321, 1383, 686, 938, 1797, 33622, 21803, 369, 11941, 281, 534, 512, 9109, 1346, 1383, 686, 1276, 310, 253, 3064, 875, 247, 31204, 29595, 285, 247, 31388, 37609, 32, 1383, 686, 1276, 403, 253, 5373, 273, 30347, 285, 2139, 581, 943, 30755, 15363, 3340, 604, 368, 452, 247, 42197, 14898, 326, 2789, 368, 1790, 512, 1388, 1048, 32, 1383, 686, 11540, 1419, 534, 7935, 310, 2876, 390, 49298, 27, 33210, 1079, 13, 16456, 11917, 1383, 686, 4509, 436, 12494, 13, 2028, 479, 672, 6393, 4962, 285, 672, 858, 12694, 574, 6393, 13, 11628, 273, 45690, 32749, 15, 5220, 13, 665, 4516, 14777, 281, 452, 12694, 689, 394, 2924, 32, 1383, 686, 7371, 5810, 5663, 281, 9087, 390, 24466, 21368, 32, 18039, 27418, 13, 24114, 14427, 1383, 686, 3378, 1677, 2505, 347, 247, 3806, 13, 7764, 42845, 1066, 690, 4278, 670, 8170, 11314, 2499, 1383, 686, 7992, 974, 253, 12696, 275, 30747, 31371, 432, 253, 2505, 40219, 686, 2402, 2620, 13946, 6671, 275, 253, 329, 6739, 40219, 686, 1276, 310, 253, 3064, 875, 6021, 285, 2867, 10474, 32, 1383, 346, 1276, 403, 690, 273, 253, 5441, 670, 253, 16365, 273, 5003, 19885, 18463, 326, 812, 5513, 352, 281, 3095, 326, 2506, 626, 871, 670, 352, 1754, 327, 436, 2505, 17295, 686, 7883, 369, 253, 11308, 33764, 273, 2389, 1383, 686, 7883, 310, 253, 3625, 5086, 39092, 327, 253, 5579, 921, 346, 510, 1594, 14, 19284, 16296, 1383, 686, 1276, 310, 253, 5347, 273, 6176, 32, 1383, 686, 1276, 403, 253, 1740, 12396, 273, 253, 807, 32, 1383, 686, 5371, 403, 253, 608, 1629, 395, 7110, 32, 1383, 686, 19735, 479, 247, 16950, 264, 1618, 273, 253, 5093, 954, 4102, 10544, 37627, 432, 23881, 40219, 686, 1276, 403, 253, 3000, 273, 3995, 1219, 314, 83, 279, 32, 1383, 686, 1276, 310, 42172, 386, 3490, 32, 1383, 686, 7371, 273, 841, 4957, 403, 9371, 323, 4983, 247, 3289, 32, 6397, 2706, 13, 2238, 1981, 13, 36767, 13, 11547, 13, 1824, 13, 8013, 8158, 20953, 13, 247, 1182, 5265, 80, 13, 9685, 10583, 13, 6906, 6079, 7356, 13, 285, 5558, 43573, 21259, 40219, 686, 17570, 479, 670, 5119, 17698, 257, 24423, 1383, 686, 4947, 1419, 1016, 273, 253, 1563, 347, 2057, 247, 6867, 5798, 1416, 390, 4370, 1416, 27, 28238, 13, 1536, 1886, 398, 13, 32726, 561, 13, 15731, 6421, 13, 401, 7112, 13, 416, 1189, 13, 418, 515, 466, 13, 36533, 13, 36172, 13, 322, 4121, 13, 31822, 1383, 686, 1276, 403, 253, 5373, 273, 23986, 32, 1383, 686, 7883, 3562, 322, 265, 482, 5720, 32, 1383, 686, 5804, 14980, 1379, 619, 2628, 1383, 686, 19735, 479, 247, 16950, 264, 1618, 273, 253, 1390, 5093, 3645, 12234, 20721, 1383, 686, 7371, 2285, 556, 22390, 427, 76, 30003, 310, 271, 9775, 326, 8631, 247, 4836, 15, 19566, 247, 2380, 326, 20420, 29141, 253, 2748, 15, 535, 50278, 187, 5013, 15768, 436, 2505, 13, 2028, 479, 253, 37131, 273, 253, 416, 489, 7564, 25749, 285, 835, 352, 369, 23179, 40219, 686, 19735, 479, 247, 1618, 273, 690, 5322, 42629, 6656, 5396, 17114, 1383, 346, 4509, 253, 10056, 2530, 13, 4908, 17750, 410, 434, 34826, 995, 686, 10639, 247, 1618, 273, 4957, 326, 403, 773, 18265, 41307, 668, 387, 12805, 1940, 1383, 686, 7371, 5579, 10684, 310, 670, 247, 23367, 2127, 275, 42408, 21706, 32, 1383, 686, 1276, 2789, 247, 5875, 6196, 32, 1383, 686, 2765, 253, 1180, 273, 20784, 24761, 12805, 1940, 556, 15, 19655, 1747, 366, 1754, 327, 4328, 275, 247, 16317, 1340, 15, 7890, 253, 5981, 2802, 11057, 273, 27134, 1163, 27036, 15385, 1383, 346, 1276, 310, 253, 1416, 273, 6893, 32782, 434, 19827, 46607, 346, 1276, 369, 15392, 418, 3681, 74, 434, 954, 8530, 789, 46607, 686, 4947, 1419, 253, 8238, 1754, 327, 1880, 597, 403, 4441, 275, 8799, 261, 285, 41395, 9821, 390, 40983, 4880, 79, 24226, 1752, 35884, 13, 3599, 22821, 5781, 711, 22827, 274, 73, 2072, 13, 353, 682, 14, 54, 16853, 13, 401, 12713, 13, 2070, 2623, 1383, 686, 1276, 2238, 273, 19600, 310, 253, 2448, 33368, 5687, 1383, 686, 11540, 1419, 534, 7935, 310, 2876, 390, 49298, 27, 2058, 10442, 1452, 13, 329, 6362, 1205, 1383, 686, 7845, 1918, 247, 2969, 1618, 273, 4606, 326, 15534, 727, 28646, 9260, 778, 417, 2226, 390, 310, 1077, 7520, 1383, 686, 2402, 690, 33622, 21803, 20721, 1383, 686, 1276, 310, 253, 3064, 875, 2451, 285, 2451, 13884, 32, 1383, 686, 1276, 310, 9666, 5927, 285, 849, 352, 310, 2905, 281, 5101, 5880, 32, 1383, 686, 2402, 1264, 25443, 407, 7252, 12967, 33739, 40219, 686, 7883, 403, 253, 1682, 4983, 36481, 47436, 273, 512, 673, 32, 1383, 686, 1276, 403, 690, 4217, 4088, 281, 1234, 321, 3014, 247, 40068, 3817, 32, 1383, 686, 15768, 253, 10056, 2708, 13, 2028, 479, 849, 1142, 22197, 2577, 18165, 13, 29544, 90, 18165, 13, 285, 41073, 11412, 18165, 11276, 24619, 556, 1912, 285, 849, 1142, 19645, 12028, 703, 574, 327, 253, 41073, 11841, 1052, 672, 703, 574, 253, 954, 19645, 12028, 323, 271, 9558, 40219, 686, 19735, 479, 1618, 273, 2022, 5248, 273, 24456, 5579, 921, 1383, 686, 2347, 858, 368, 10347, 323, 634, 2136, 1083, 7324, 32, 1383, 686, 7371, 2448, 4007, 4620, 327, 247, 581, 14, 35523, 6007, 32, 1383, 686, 2513, 352, 4999, 281, 1718, 275, 19487, 32, 1383, 346, 4509, 253, 10056, 2530, 13, 4908, 253, 4454, 273, 500, 1403, 1240, 7694, 267, 3532, 73, 579, 434, 4651, 15, 16925, 33573, 731, 342, 247, 39169, 17295, 686, 1276, 310, 253, 1682, 2952, 272, 281, 897, 327, 36557, 32, 1383, 686, 1276, 3698, 943, 309, 4489, 32, 1383, 686, 7161, 513, 17267, 3153, 32, 1383, 686, 7371, 273, 253, 1563, 476, 8778, 27, 12621, 13, 12120, 13, 5798, 13, 4370, 13, 1113, 13, 25816, 13, 28141, 13, 13696, 13, 1721, 13, 38985, 13, 22676, 32, 1383, 686, 13325, 247, 16950, 1618, 273, 690, 1027, 4088, 326, 247, 5772, 476, 1056, 690, 2583, 1383, 686, 7883, 1912, 247, 5328, 19358, 12936, 34184, 275, 253, 820, 89, 264, 1740, 32, 1383, 346, 1276, 310, 388, 8185, 434, 24588, 46607, 686, 17570, 479, 1880, 841, 13225, 403, 2876, 264, 390, 49298, 27, 330, 6646, 13, 330, 3445, 932, 13, 11835, 2134, 13, 3972, 81, 1383, 686, 15768, 436, 3806, 2505, 670, 25291, 418, 1836, 1777, 74, 13, 1309, 534, 1107, 369, 253, 399, 13010, 1315, 33040, 417, 253, 22583, 5675, 1315, 33040, 275, 253, 1982, 32, 1383, 686, 4509, 253, 10056, 2530, 13, 4908, 253, 2264, 1885, 11784, 285, 2462, 40534, 58, 3116, 970, 5981, 27, 37189, 89, 94, 1885, 11784, 342, 551, 89, 6, 94, 40534, 58, 3116, 61, 79, 61, 79, 13443, 11784, 273, 370, 27865, 15, 20, 3041, 275, 253, 7002, 7150, 13, 9999, 8255, 6, 807, 14, 1189, 14, 2913, 3116, 61, 79, 8494, 1776, 3045, 17973, 273, 370, 20, 15, 24, 6494, 13, 9999, 6480, 6, 807, 14, 1189, 14, 2913, 3116, 61, 79, 24, 13, 25, 1619, 2264, 6383, 61, 79, 8695, 11784, 17302, 2281, 273, 22287, 33760, 79, 24070, 6383, 342, 28417, 1249, 14, 7791, 1885, 11784, 3687, 685, 370, 18, 3041, 61, 79, 14897, 1025, 247, 370, 19, 15, 17, 6494, 5739, 1234, 25125, 2086, 1383, 686, 5430, 476, 359, 3657, 253, 1682, 8281, 42303, 32, 1383, 686, 1276, 310, 247, 6858, 5798, 1925, 32, 1383, 686, 19735, 479, 5697, 849, 309, 812, 2489, 625, 19303, 40219, 686, 38, 1266, 84, 403, 8214, 841, 1897, 15, 14482, 309, 10581, 619, 1211, 29466, 3185, 32, 1383, 686, 1276, 403, 690, 273, 253, 1841, 281, 871, 670, 1457, 2816, 3228, 32, 1383, 686, 1276, 310, 253, 4495, 273, 1495, 32, 1383, 686, 9395, 247, 43240, 14, 2388, 8613, 7450, 14, 23290, 14, 1851, 84, 30755, 10934, 326, 309, 476, 513, 721, 2069, 247, 2129, 40219, 686, 7883, 310, 6086, 416, 15, 51, 15, 8698, 32, 1383, 686, 2513, 436, 19971, 13, 14623, 13, 390, 1097, 32, 1383, 686, 4967, 858, 6086, 44769, 11301, 7168, 253, 26083, 32, 1383, 686, 19735, 479, 247, 1618, 273, 253, 1390, 2620, 346, 14461, 17845, 4498, 3, 22197, 2577, 20721, 313, 6309, 2028, 479, 253, 4498, 1416, 285, 253, 9591, 9558, 481, 1383, 686, 11540, 1419, 285, 13366, 2319, 1740, 773, 12467, 942, 668, 323, 3576, 12674, 2493, 342, 253, 5759, 15, 3166, 368, 1158, 247, 4007, 3198, 247, 1175, 2954, 342, 5759, 281, 320, 271, 3576, 4007, 32, 1383, 686, 1276, 3470, 452, 3569, 4676, 5608, 275, 2448, 8672, 32, 2903, 405, 253, 4757, 273, 1016, 4809, 273, 253, 530, 15, 52, 15, 3569, 4676, 1128, 783, 3128, 275, 2208, 13, 253, 3128, 347, 8889, 13, 285, 3128, 275, 253, 46866, 40219, 686, 7883, 310, 27118, 43, 32, 1383, 686, 39, 1194, 39, 369, 4232, 275, 534, 807, 32, 1383, 686, 1276, 403, 253, 8946, 3054, 273, 2647, 32, 1383, 346, 1276, 434, 253, 3064, 875, 247, 26994, 13, 247, 47826, 285, 247, 23104, 46607, 686, 15545, 327, 253, 3806, 30003, 310, 271, 9775, 326, 8631, 247, 4836, 15, 19566, 247, 2380, 326, 20420, 29141, 253, 2748, 15, 535, 50278, 187, 5013, 1276, 403, 690, 1846, 1894, 3510, 275, 399, 7, 37, 32, 1383, 686, 38997, 12694, 3719, 44596, 27001, 327, 247, 5439, 5147, 1925, 247, 260, 682, 39226, 1452, 387, 1383, 686, 17570, 479, 670, 253, 7544, 21581, 5377, 28002, 9728, 2457, 32, 1383, 686, 4967, 310, 16916, 6112, 247, 4633, 1659, 281, 3153, 1383, 686, 1276, 310, 247, 15120, 34057, 326, 651, 320, 794, 281, 755, 715, 32, 1383, 686, 7371, 3085, 1912, 2709, 15913, 17937, 18165, 32, 61, 79, 34, 15, 32111, 7612, 74, 500, 16189, 61, 79, 35, 15, 21806, 472, 5582, 61, 79, 36, 15, 330, 15, 42, 15, 37, 4880, 79, 37, 15, 380, 990, 14505, 403, 11704, 407, 253, 4498, 329, 6683, 37723, 388, 404, 48905, 40408, 40219, 346, 1276, 310, 253, 3064, 875, 697, 285, 352, 434, 46607, 686, 10639, 247, 2926, 835, 247, 28806, 41217, 271, 9488, 15985, 15, 380, 2926, 943, 452, 247, 5068, 13, 4766, 285, 990, 40219, 686, 15768, 841, 33295, 670, 253, 27876, 17994, 13, 835, 285, 672, 369, 352, 6138, 32, 1383, 686, 1276, 310, 253, 1682, 1039, 281, 2746, 247, 747, 5798, 32, 1383, 686, 1276, 310, 20407, 730, 19177, 32, 1383, 686, 1276, 403, 690, 4633, 3186, 14917, 32, 1383, 686, 12822, 15, 8524, 1906, 4916, 374, 2109, 3416, 281, 2186, 253, 1501, 846, 1383, 686, 1276, 310, 247, 6867, 13986, 4062, 672, 247, 6858, 310, 5686, 32, 1383, 686, 2347, 2223, 403, 253, 17165, 2918, 32, 1383, 686, 2347, 1142, 11321, 858, 6423, 3277, 41319, 308, 5746, 2610, 1056, 32, 1383, 686, 5092, 368, 5583, 794, 4712, 281, 4264, 275, 16916, 275, 5768, 32, 1383, 686, 4967, 403, 9449, 10169, 594, 4633, 275, 6176, 32, 1383, 686, 1276, 310, 581, 273, 634, 7583, 3295, 32, 1383, 686, 2347, 281, 2489, 247, 1175, 2285, 4760, 32, 1383, 686, 7371, 310, 247, 3417, 273, 6773, 32, 12605, 1240, 390, 12605, 22876, 1383, 686, 1276, 943, 309, 1908, 672, 18000, 875, 5637, 1113, 390, 3678, 1113, 32, 1383, 686, 1276, 310, 17633, 20233, 32, 1383, 686, 4045, 24806, 5663, 275, 17753, 14, 4826, 30961, 2399, 32, 1383, 686, 18837, 359, 755, 8314, 273, 2675, 3420, 32, 1383, 686, 6723, 627, 667, 42830, 14, 19771, 18075, 281, 1424, 78, 318, 32, 1383, 686, 10002, 3562, 253, 806, 45981, 8800, 32, 1383, 686, 4509, 253, 10056, 2530, 13, 4908, 253, 767, 4948, 273, 2341, 3700, 285, 1618, 731, 275, 29093, 40219, 686, 1276, 403, 253, 2620, 46861, 84, 273, 1457, 2816, 3228, 32, 1383, 686, 4967, 513, 309, 452, 35932, 4707, 32, 1383, 686, 1276, 403, 2067, 4088, 281, 1978, 247, 50245, 10000, 32, 1383, 686, 1276, 403, 1841, 309, 476, 513, 281, 1361, 479, 4868, 1805, 327, 247, 1071, 32, 1383, 686, 2347, 1142, 20581, 403, 627, 275, 253, 9666, 985, 32, 1383, 686, 1276, 310, 10721, 253, 16778, 32, 1383, 686, 10795, 8141, 4492, 1347, 973, 323, 2444, 327, 253, 3971, 32, 1383, 346, 26845, 434, 3101, 574, 495, 2151, 15, 5761, 273, 731, 403, 6393, 285, 29835, 15, 1737, 310, 253, 1416, 273, 253, 1390, 1429, 46607, 686, 4509, 253, 2708, 2505, 13, 1618, 253, 1755, 495, 23168, 273, 253, 26165, 356, 87, 324, 3096, 1464, 13, 342, 253, 954, 34067, 40219, 686, 1276, 310, 1805, 27, 48669, 312, 37364, 342, 14354, 390, 342, 659, 2040, 25058, 3736, 1383, 686, 3039, 858, 253, 15732, 3660, 11075, 32, 1383, 686, 1276, 310, 352, 751, 1146, 253, 1390, 1436, 327, 6149, 32, 1383, 686, 4947, 1419, 253, 1563, 3578, 9005, 3510, 407, 28036, 1163, 61, 79, 23644, 14734, 13, 37887, 19901, 13, 7962, 3144, 13, 37904, 406, 360, 19901, 13, 37887, 335, 251, 6785, 316, 13, 49608, 2399, 10287, 3144, 13, 4082, 15683, 360, 19901, 13, 4082, 287, 1344, 3144, 13, 21913, 493, 83, 3144, 13, 21913, 406, 360, 19901, 13, 1383, 346, 4476, 19268, 247, 36108, 7156, 368, 1849, 574, 17295, 686, 1276, 310, 49347, 32, 1383, 686, 5371, 84, 3058, 281, 1056, 247, 3350, 90, 6451, 32, 61, 79, 61, 79, 23804, 61, 79, 348, 20471, 61, 23713, 2153, 61, 2224, 16734, 61, 2369, 300, 61, 79, 11714, 61, 2649, 319, 293, 61, 79, 2009, 1257, 1452, 61, 4160, 621, 61, 79, 13606, 1383, 686, 1276, 403, 253, 3625, 12696, 285, 37484, 326, 564, 715, 11308, 12398, 32, 1383, 686, 17570, 479, 604, 841, 403, 8238, 390, 4343, 27, 11268, 13, 7785, 13, 12300, 285, 6176, 1383, 686, 1276, 310, 247, 15670, 56, 32, 1383, 686, 1576, 281, 6635, 13978, 1383, 686, 15768, 436, 3806, 2505, 327, 253, 8869, 273, 1399, 66, 2244, 13, 835, 858, 253, 40898, 35282, 32, 1383, 686, 688, 253, 2530, 2505, 2709, 6667, 273, 247, 7982, 403, 1677, 15, 46685, 253, 2626, 1650, 273, 247, 7982, 40219, 686, 1276, 2238, 273, 3417, 310, 253, 34999, 29173, 270, 5021, 1383, 686, 688, 29992, 13, 2139, 310, 253, 19538, 21392, 387, 7457, 2792, 3185, 273, 4314, 32, 1383, 686, 1276, 310, 253, 8004, 1375, 275, 253, 5106, 32, 1383, 686, 17570, 479, 1880, 841, 952, 403, 25795, 2458, 390, 17812, 27, 11007, 9384, 13, 15273, 14089, 13, 6270, 19655, 5650, 13, 9915, 11445, 266, 13, 2516, 27614, 78, 13, 33208, 444, 1378, 91, 13, 611, 19482, 19995, 1383, 686, 15768, 247, 3806, 2505, 670, 8293, 6884, 14182, 13, 2028, 479, 665, 253, 2022, 14142, 403, 285, 752, 9503, 597, 4546, 40219, 686, 1276, 1057, 44442, 15082, 2097, 32, 1383, 686, 7883, 310, 12843, 313, 14557, 6876, 6177, 1383, 686, 19735, 479, 253, 1618, 273, 1755, 884, 11321, 4439, 275, 253, 1655, 45660, 13, 407, 309, 7414, 35, 13716, 15, 31512, 27, 13927, 313, 18992, 10421, 10, 36485, 1383, 686, 10639, 247, 2159, 2926, 670, 247, 1436, 665, 41217, 247, 8763, 2316, 275, 616, 2419, 15, 380, 2926, 943, 2486, 247, 7484, 19152, 285, 247, 2590, 6064, 387, 253, 990, 40219, 686, 1276, 310, 253, 954, 1774, 2181, 323, 1569, 10473, 275, 247, 39721, 1083, 32, 1383, 346, 1276, 403, 253, 16108, 273, 42654, 12561, 32, 2615, 368, 2303, 634, 3662, 281, 247, 6867, 2448, 285, 6266, 30003, 310, 271, 9775, 326, 8631, 247, 4836, 15, 19566, 247, 2380, 326, 20420, 29141, 253, 2748, 15, 535, 50278, 187, 9855, 1909, 591, 10246, 276, 25658, 434, 8063, 752, 943, 253, 2020, 273, 17818, 2341, 13, 2442, 2341, 285, 4812, 2341, 320, 46607, 686, 19735, 247, 1618, 273, 253, 1027, 9351, 273, 3289, 21510, 326, 2226, 40219, 686, 7371, 1375, 275, 253, 1982, 556, 253, 4585, 24820, 32, 1383, 686, 17570, 479, 1880, 841, 403, 8458, 390, 6582, 3052, 7604, 76, 3104, 27, 27876, 10031, 495, 13, 27314, 3094, 6992, 263, 13, 4661, 11170, 30947, 13, 34379, 8874, 636, 13, 5002, 7728, 545, 6147, 13, 4974, 29883, 16009, 250, 4019, 13, 13153, 8141, 3243, 13, 11553, 6201, 1258, 7068, 13, 5003, 8174, 35471, 968, 1383, 686, 1276, 310, 27665, 32, 1383, 686, 1276, 4592, 275, 9002, 17764, 1383, 686, 4967, 9384, 1639, 6785, 272, 594, 4633, 32, 1680, 5561, 21266, 247, 1175, 9678, 32, 1383, 686, 4947, 1419, 253, 4454, 1754, 327, 1880, 597, 1265, 342, 15956, 47, 457, 390, 2802, 46, 457, 61, 79, 47, 4306, 13, 18150, 74, 13, 28331, 13, 4744, 13, 6897, 21206, 13, 13104, 1383, 686, 5804, 368, 4496, 921, 690, 4278, 5001, 253, 17325, 3085, 378, 15188, 777, 12079, 970, 1677, 2505, 347, 247, 3806, 1383, 686, 2765, 347, 1142, 3510, 273, 11072, 10562, 347, 368, 476, 1383, 686, 1276, 403, 2620, 10995, 4088, 281, 897, 2929, 32, 1383, 686, 1276, 403, 690, 4088, 281, 33150, 29757, 40777, 292, 32, 1383, 686, 50, 27, 3052, 309, 13410, 323, 247, 48696, 6178, 273, 247, 19314, 14, 746, 12538, 13, 285, 604, 594, 13, 534, 581, 32, 1383, 686, 2347, 3477, 285, 4999, 310, 352, 281, 8171, 253, 36609, 327, 634, 1113, 407, 4834, 32, 1383, 686, 18179, 309, 7171, 247, 25872, 275, 247, 3819, 32, 1383, 686, 1276, 369, 34473, 23259, 32, 1383, 686, 7371, 310, 247, 2728, 32, 15123, 14894, 13, 27675, 13, 13877, 13, 24482, 13, 3697, 20224, 571, 13, 39942, 20521, 13, 33649, 1383, 686, 20655, 16495, 29628, 4391, 253, 24591, 273, 346, 510, 12688, 310, 247, 1535, 1301, 3, 281, 346, 510, 12688, 310, 247, 3, 752, 32, 1383, 686, 1394, 403, 247, 2872, 15245, 273, 247, 6684, 2586, 15, 19566, 247, 4857, 275, 1679, 685, 7783, 3000, 281, 247, 11547, 13, 27321, 634, 4468, 689, 13583, 23585, 273, 35156, 8138, 285, 281, 5195, 11891, 273\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n6598, 13, 49557, 19580, 15916, 13, 10852, 353, 797, 91, 13, 29183, 13, 2516, 6212, 1383, 686, 2347, 556, 25856, 41098, 23509, 4195, 1250, 1223, 21855, 253, 5003, 10765, 7584, 398, 32, 1383, 686, 15768, 253, 1563, 12494, 670, 253, 2892, 273, 10086, 383, 9195, 13, 534, 369, 253, 806, 1629, 656, 33205, 468, 285, 672, 369, 352, 1973, 32, 1383, 686, 2347, 1142, 5753, 1057, 4276, 86, 452, 327, 253, 3759, 793, 790, 32, 1383, 686, 19735, 479, 247, 1618, 273, 4370, 4454, 1383, 686, 4947, 1419, 1016, 273, 253, 1563, 347, 247, 10717, 3448, 390, 417, 27, 418, 11135, 13, 20233, 13, 9615, 468, 11780, 13, 367, 9490, 13, 16048, 26300, 13, 17231, 1179, 13, 42413, 36413, 13, 47833, 371, 1832, 13, 1219, 32651, 13, 8595, 13, 330, 44653, 3094, 661, 719, 88, 13, 3035, 11170, 13, 24114, 71, 1807, 1383, 686, 15545, 327, 253, 1677, 10056, 2028, 479, 665, 34741, 2453, 1346, 403, 32, 1383, 686, 45615, 34896, 281, 18088, 3066, 14645, 10947, 13, 1650, 7102, 1383, 686, 4947, 1419, 1016, 273, 841, 1113, 16596, 347, 2057, 5112, 13, 5685, 13, 2448, 13, 6692, 13, 390, 643, 27, 24766, 316, 13, 8305, 25581, 74, 13, 1219, 6855, 12300, 80, 13, 657, 56, 13, 34379, 13, 15509, 13, 330, 4905, 257, 1383, 686, 1276, 1057, 8784, 12706, 1462, 323, 1383, 686, 1276, 403, 253, 5018, 309, 943, 1379, 281, 2489, 247, 941, 16518, 323, 7880, 43240, 32, 1383, 686, 11540, 1419, 534, 7935, 310, 2876, 390, 49298, 27, 2263, 1947, 13, 6813, 6968, 86, 1383, 686, 4967, 556, 4255, 644, 247, 2603, 273, 16305, 4102, 32, 1383, 686, 7371, 310, 247, 3417, 273, 6773, 32, 3545, 348, 390, 14929, 463, 1383, 686, 1276, 310, 247, 1175, 4868, 281, 5310, 275, 15692, 32, 1383, 686, 7992, 974, 253, 12696, 2424, 281, 10347, 1149, 317, 312, 1306, 432, 253, 2505, 15, 16925, 33573, 731, 342, 247, 39169, 40219, 686, 1276, 2789, 24639, 247, 1755, 22777, 12095, 32, 1383, 686, 1276, 403, 12645, 323, 690, 5284, 8238, 281, 4143, 40219, 686, 15768, 436, 12494, 670, 1027, 3510, 273, 2927, 13, 849, 403, 5130, 3510, 27948, 285, 2139, 32, 1383, 686, 4967, 513, 5753, 751, 281, 1132, 10188, 79, 614, 594, 1199, 32, 1383, 686, 7130, 281, 436, 12494, 670, 12156, 44470, 13, 849, 1048, 403, 5086, 6782, 44470, 275, 17249, 32, 1383, 686, 1276, 403, 253, 1682, 5098, 273, 253, 655, 394, 5331, 32, 1383, 30003, 310, 271, 9775, 326, 8631, 247, 4836, 15, 19566, 247, 2380, 326, 20420, 29141, 253, 2748, 15, 535, 50278, 187, 5013, 1276, 310, 253, 8588, 751, 275, 1457, 8911, 32, 1383, 686, 7992, 974, 253, 28966, 273, 1016, 1777, 763, 12699, 23378, 275, 436, 12494, 285, 1618, 275, 16950, 264, 5981, 40219, 686, 1276, 310, 247, 8462, 394, 356, 37173, 16260, 32, 1383, 686, 7992, 974, 253, 4060, 273, 253, 2165, 13, 253, 1416, 273, 697, 13722, 285, 253, 1416, 273, 253, 2022, 1894, 285, 4858, 731, 407, 247, 39169, 40219, 346, 1276, 434, 253, 10336, 275, 36685, 9382, 1198, 3409, 1007, 751, 46607, 686, 2513, 14980, 247, 2495, 281, 16457, 32, 1383, 686, 1276, 310, 9233, 10339, 32, 1383, 686, 4509, 253, 10056, 2530, 13, 4908, 253, 8083, 26949, 326, 18387, 3684, 5427, 15, 16925, 33573, 731, 342, 247, 39169, 40219, 686, 4947, 1419, 253, 2708, 4957, 1754, 327, 253, 1511, 273, 1655, 597, 897, 13, 11940, 390, 9087, 4880, 79, 10847, 272, 273, 19978, 13, 7458, 13, 44312, 13, 2329, 1617, 398, 1383, 686, 1276, 807, 858, 3645, 3660, 374, 990, 32, 1383, 686, 19735, 479, 247, 3410, 273, 1264, 273, 9943, 29301, 40219, 686, 1276, 1057, 352, 1379, 281, 2489, 271, 33667, 11572, 275, 253, 5106, 32, 1383, 686, 1276, 403, 1264, 8238, 327, 253, 9268, 8852, 273, 7758, 32, 1383, 686, 17570, 479, 1880, 1016, 273, 841, 21114, 48520, 403, 275, 13338, 390, 5002, 27, 7967, 25743, 13, 353, 3681, 837, 13, 3729, 8141, 13, 657, 647, 13, 26989, 257, 13, 19906, 321, 504, 1383, 686, 10195, 952, 403, 16063, 386, 281, 1718, 5300, 2584, 32103, 1066, 390, 1014, 40310, 253, 2538, 273, 7952, 1818, 984, 597, 1158, 352, 1057, 417, 1056, 3282, 604, 760, 247, 1355, 5110, 273, 253, 3072, 1718, 84, 5300, 15, 1583, 1158, 352, 310, 11339, 281, 513, 594, 1919, 4130, 5821, 281, 513, 594, 390, 310, 6726, 407, 3646, 15, 1737, 310, 253, 1921, 326, 690, 952, 1335, 513, 594, 32, 1383, 686, 1672, 19104, 11265, 1180, 272, 985, 273, 7422, 3736, 1383, 346, 19735, 479, 247, 1618, 273, 253, 1390, 209, 1271, 76, 366, 891, 1849, 1620, 2326, 995, 686, 4947, 1419, 253, 2708, 15991, 1754, 327, 616, 3626, 3520, 1375, 4880, 79, 26042, 13, 36091, 13, 2434, 3803, 13, 473, 38190, 13, 427, 10682, 13, 9002, 1383, 686, 1276, 403, 690, 6867, 5810, 273, 10283, 1443, 70, 16581, 32, 1383, 686, 26021, 247, 1691, 350, 13, 752, 643, 15692, 15202, 651, 36433, 323, 247, 8133, 9980, 32, 1383, 686, 2347, 513, 309, 4993, 253, 3948, 273, 619, 1113, 32, 1383, 686, 1276, 5997, 271, 33667, 12916, 281, 8778, 32, 1383, 686, 11540, 1419, 432, 436, 1618, 534, 403, 330, 1402, 6836, 260, 1402, 251, 9830, 27, 36767, 39438, 11381, 13, 1218, 12610, 41987, 282, 9461, 13, 665, 2695, 466, 4204, 261, 447, 13, 19445, 268, 11114, 11105, 13, 27771, 318, 14863, 13, 17649, 26996, 13, 23355, 3827, 8862, 13, 5315, 10510, 74, 285, 12173, 13, 8574, 2829, 4797, 13, 8034, 29197, 13, 12649, 33830, 460, 13, 24455, 12840, 25308, 13, 6150, 4759, 1383, 686, 1276, 310, 32064, 4662, 16967, 274, 8530, 323, 32, 1383, 686, 11540, 1419, 534, 1113, 11662, 310, 5685, 390, 2448, 27, 27876, 13, 13173, 76, 27418, 31449, 1383, 686, 26068, 273, 1264, 1027, 9830, 326, 34939, 19519, 403, 2223, 1383, 686, 2347, 513, 368, 1056, 253, 1682, 703, 20640, 1397, 3376, 32, 1383, 686, 11888, 253, 767, 33295, 2708, 13, 672, 369, 253, 27485, 775, 16973, 38004, 4530, 273, 10310, 11420, 13, 285, 672, 858, 352, 1132, 275, 17111, 323, 253, 806, 673, 32, 1383, 686, 3039, 310, 253, 806, 1388, 273, 5768, 1384, 1508, 32, 1383, 686, 261, 247, 5249, 275, 1345, 15890, 247, 1175, 1039, 281, 564, 1383, 686, 3039, 12478, 247, 2419, 13, 13213, 907, 1016, 273, 253, 1563, 347, 346, 316, 4085, 3, 390, 346, 1439, 4217, 1381, 378, 4461, 13, 353, 412, 13, 36495, 8343, 13, 7889, 13, 399, 4580, 13, 35540, 9251, 13, 11431, 1383, 686, 1276, 22290, 403, 327, 253, 7908, 273, 253, 16333, 32, 1383, 686, 1276, 310, 247, 14139, 8241, 528, 1716, 6259, 29088, 313, 4061, 34, 6177, 1383, 686, 15545, 327, 253, 1840, 10056, 30215, 378, 693, 438, 2458, 1754, 327, 616, 18499, 2112, 342, 9056, 26026, 275, 247, 24312, 15, 27604, 253, 1543, 275, 247, 39169, 9070, 5981, 40219, 686, 1276, 310, 253, 24984, 1416, 323, 399, 3938, 8451, 32, 1383, 686, 26068, 273, 247, 12190, 273, 4088, 326, 368, 812, 4076, 247, 2419, 40219, 686, 4967, 513, 359, 1756, 767, 5239, 273, 9970, 32, 1383, 686, 1276, 310, 253, 1682, 7758, 10030, 1383, 686, 1276, 403, 253, 747, 818, 411, 23023, 37665, 27, 1383, 686, 5371, 310, 42295, 32, 1383, 686, 3039, 369, 253, 806, 11547, 3863, 275, 253, 1986, 2077, 32, 1383, 686, 10639, 247, 2159, 5700, 327, 849, 281, 2572, 634, 14512, 281, 3330, 247, 29992, 2165, 40219, 686, 1276, 3510, 273, 22819, 50, 476, 368, 755, 275, 3729, 9756, 32, 1383, 686, 5804, 368, 26799, 323, 479, 275, 642, 625, 685, 1264, 33295, 253, 1984, 13, 346, 510, 11573, 10797, 2064, 16574, 995, 407, 11327, 555, 16617, 468, 32, 1383, 686, 1276, 1057, 270, 4904, 14, 4924, 1599, 32, 1383, 686, 2437, 1419, 253, 1563, 347, 9001, 4632, 643, 4712, 27, 25154, 13, 5015, 13, 4028, 13, 3515, 13, 18792, 13, 5842, 13, 15692, 13, 5140, 13, 43955, 13, 17120, 13, 1061, 356, 77, 2821, 13, 16893, 13, 2970, 3563, 1383, 686, 3039, 369, 7854, 1758, 657, 656, 1041, 19306, 32518, 5686, 32, 1383, 686, 2513, 352, 1805, 281, 21114, 390, 8762, 4697, 32, 1383, 686, 1276, 310, 5787, 16775, 32, 1383, 346, 5371, 369, 253, 1533, 434, 806, 7141, 14, 14996, 4382, 46607, 686, 1276, 943, 309, 513, 604, 619, 16556, 556, 644, 41765, 32, 1383, 686, 19735, 479, 1618, 273, 1390, 884, 37627, 273, 1982, 40219, 686, 10639, 247, 4864, 2892, 273, 253, 1533, 347, 2183, 432, 253, 8668, 273, 247, 2502, 5308, 40219, 686, 2402, 973, 1929, 11321, 342, 6086, 1639, 80, 2153, 40219, 686, 19735, 479, 247, 2159, 16950, 264, 1618, 273, 253, 2234, 2792, 670, 253, 7529, 6978, 30003, 310, 271, 9775, 326, 8631, 247, 4836, 15, 19566, 247, 2380, 326, 20420, 29141, 253, 2748, 15, 535, 50278, 187, 5013, 10639, 253, 660, 1792, 383, 2159, 2926, 1896, 1383, 686, 1276, 310, 1057, 253, 6114, 1957, 327, 253, 1986, 2077, 273, 3968, 7908, 32, 1383, 686, 7992, 974, 253, 4454, 273, 253, 33663, 273, 2588, 12354, 330, 15, 39, 15, 36, 432, 253, 2505, 32, 1383, 686, 1276, 310, 253, 2605, 273, 5259, 24703, 8881, 13, 347, 908, 275, 1982, 8881, 31067, 32, 1383, 686, 1276, 310, 247, 5899, 36496, 390, 24877, 76, 32, 1383, 686, 1276, 310, 253, 2022, 2847, 273, 8762, 32, 1383, 686, 2347, 651, 368, 564, 670, 4560, 271, 10230, 275, 247, 747, 2846, 32, 1383, 686, 1276, 310, 247, 28407, 32, 1383, 686, 4967, 310, 1527, 2603, 594, 1774, 32, 1383, 686, 7883, 310, 6277, 13268, 32, 1383, 686, 7910, 1355, 7888, 403, 9070, 407, 247, 4181, 273, 1264, 6574, 15, 496, 253, 5928, 273, 247, 4980, 39063, 11319, 13, 1158, 273, 2620, 4088, 323, 21251, 281, 14556, 6431, 8169, 40219, 686, 2347, 1142, 11559, 1057, 247, 16819, 12609, 452, 32, 1383, 686, 1276, 310, 247, 1175, 8149, 5700, 32, 1383, 686, 7371, 273, 841, 403, 5074, 27, 17026, 13, 23244, 13, 4370, 13, 6194, 13, 33667, 13, 5798, 13, 39121, 13, 10464, 13, 9461, 13, 9735, 40219, 686, 1276, 310, 253, 3064, 875, 6021, 2891, 285, 5347, 15988, 2891, 275, 253, 5106, 32, 1383, 686, 1276, 310, 247, 1175, 1659, 281, 2619, 598, 28191, 2739, 672, 6276, 432, 15939, 3145, 281, 17687, 3062, 32, 1383, 686, 6723, 1781, 3448, 3210, 2686, 7060, 751, 7497, 32, 1383, 686, 1276, 2846, 275, 253, 1533, 310, 12258, 347, 1907, 253, 6253, 1180, 273, 5702, 285, 8530, 919, 15618, 285, 20183, 4697, 439, 6143, 32, 1383, 686, 17570, 479, 690, 2234, 2792, 670, 309, 1433, 84, 432, 1677, 2505, 40219, 686, 1276, 310, 380, 6203, 4683, 380, 50012, 32, 1383, 686, 1276, 310, 17231, 1179, 32, 1383, 686, 4967, 310, 38075, 1533, 5500, 310, 594, 4633, 32, 1383, 686, 1276, 403, 2620, 3000, 326, 13882, 28705, 342, 1708, 32, 1383, 686, 2347, 1057, 253, 3276, 2818, 253, 3885, 273, 3590, 32, 1383, 686, 688, 253, 17478, 1984, 2962, 36858, 285, 24756, 12133, 13, 665, 403, 36858, 285, 24756, 12133, 4907, 846, 32, 1383, 686, 1276, 310, 247, 3168, 4697, 32, 1383, 686, 1276, 310, 11528, 32, 1383, 686, 2402, 690, 273, 253, 1682, 4583, 22555, 5098, 1383, 686, 7845, 1918, 247, 16950, 264, 1618, 273, 752, 31167, 253, 8801, 9753, 13193, 1383, 686, 7883, 369, 253, 43982, 323, 253, 3961, 31933, 32, 1383, 686, 7883, 403, 690, 3484, 387, 45998, 11005, 84, 275, 443, 610, 567, 527, 263, 2419, 326, 497, 275, 253, 1072, 807, 347, 11643, 28753, 32, 1383, 686, 1276, 310, 1605, 15723, 32, 1383, 686, 1276, 3085, 310, 7478, 347, 253, 806, 3085, 275, 253, 1533, 342, 271, 7094, 2806, 5248, 32, 1383, 686, 4967, 513, 690, 12981, 1158, 326, 18120, 513, 417, 2647, 1199, 323, 12674, 6132, 6973, 32, 6049, 513, 643, 12981, 285, 24432, 1158, 326, 18120, 513, 2647, 32, 1383, 686, 1276, 310, 37358, 2234, 12609, 32, 1383, 686, 1276, 497, 253, 20801, 273, 253, 2448, 10109, 3660, 32, 1383, 686, 11540, 1419, 534, 7935, 310, 2876, 390, 5534, 16668, 27, 775, 2259, 90, 13, 45010, 9584, 1383, 686, 1276, 310, 20242, 32, 1383, 686, 56, 698, 281, 897, 367, 21177, 643, 685, 4028, 1383, 686, 4967, 310, 20055, 19845, 13, 14070, 247, 1270, 1659, 281, 4143, 323, 18125, 32, 1383, 686, 19735, 479, 247, 1618, 273, 9279, 326, 403, 8862, 3295, 1383, 686, 2402, 1643, 16038, 12192, 285, 5609, 323, 4715, 1383, 686, 1276, 310, 581, 7565, 1388, 32, 1383, 686, 7371, 403, 253, 22583, 5074, 327, 6149, 32, 1383, 686, 10639, 247, 4864, 10056, 38746, 8450, 273, 247, 2021, 774, 406, 839, 29549, 32, 1383, 686, 26068, 273, 2620, 3000, 326, 13882, 28705, 342, 28049, 1383, 686, 74, 971, 281, 6008, 33696, 15, 752, 2238, 273, 2739, 275, 619, 6196, 943, 891, 2624, 562, 32, 6000, 1201, 309, 452, 247, 39191, 13, 42463, 13, 22534, 13, 22462, 13, 285, 8574, 40219, 686, 17570, 479, 849, 417, 281, 30755, 32, 1383, 686, 7161, 310, 253, 24253, 732, 28759, 261, 11037, 3770, 40014, 6773, 3839, 1119, 32, 1383, 686, 15768, 436, 12494, 670, 253, 2846, 273, 21924, 13, 752, 403, 253, 2022, 39719, 273, 253, 2846, 32, 1383, 686, 7992, 974, 253, 12095, 4343, 273, 253, 23089, 8432, 1073, 284, 47915, 432, 253, 2505, 15, 16925, 33573, 731, 342, 247, 39169, 40219, 686, 4476, 19268, 247, 2098, 323, 6276, 432, 7758, 281, 1457, 2816, 342, 690, 1270, 5053, 281, 3523, 342, 2448, 9541, 40219, 686, 1276, 310, 253, 2022, 2891, 5649, 273, 247, 28230, 36496, 285, 24877, 44, 32, 1383, 686, 1276, 403, 690, 2234, 2616, 281, 1908, 275, 13887, 247, 1659, 281, 3153, 32, 1383, 686, 1276, 943, 320, 253, 1682, 13216, 275, 247, 637, 1383, 346, 7883, 310, 253, 39092, 275, 733, 12584, 2263, 87, 2610, 434, 496, 22772, 37665, 46607, 686, 10639, 247, 5702, 4579, 281, 619, 32084, 7004, 1880, 309, 476, 1056, 619, 8845, 7830, 1458, 1897, 3563, 1735, 1770, 1383, 686, 19735, 479, 247, 1618, 273, 3000, 326, 403, 2330, 342, 27477, 40219, 686, 7371, 3128, 5769, 8302, 369, 629, 273, 32, 1383, 686, 18837, 309, 564, 281, 253, 11600, 390, 14700, 323, 619, 1735, 18125, 32, 1383, 686, 688, 3806, 281, 1677, 2505, 13, 5838, 690, 2792, 670, 40566, 257, 1952, 7121, 1383, 686, 3039, 952, 564, 281, 401, 1757, 67, 23616, 372, 20840, 860, 261, 372, 6512, 14052, 281, 755, 247, 2469, 293, 372, 295, 682, 13, 849, 513, 952, 3798, 6008, 352, 32, 1244, 752, 310, 253, 13612, 40219, 686, 7371, 403, 690, 8530, 8514, 261, 1100, 432, 17549, 284, 14644, 13, 5427, 32, 1383, 686, 1276, 310, 253, 19305, 1876, 14, 16043, 2165, 32, 1383, 686, 7371, 273, 253, 1563, 403, 7744, 908, 347, 8290, 4454, 27, 13187, 13, 44339, 13, 20189, 13, 6393, 13, 20222, 13, 38150, 13, 2516, 13, 378, 8976, 13, 18322, 13, 5490, 13, 18063, 266, 13, 22723, 13, 19436, 13, 30003, 310, 271, 9775, 326, 8631, 247, 4836, 15, 19566, 247, 2380, 326, 20420, 29141, 253, 2748, 15, 535, 50278, 187, 5013, 1276, 310, 253, 2022, 2847, 273, 25033, 32, 1383, 686, 7992, 974, 512, 273, 253, 12282, 5393, 275, 436, 12494, 285, 1618, 731, 970, 29093, 275, 253, 5981, 551, 6958, 94, 428, 551, 11185, 94, 1383, 686, 7371, 310, 271, 21474, 390, 247, 7982, 6814, 27, 3174, 13, 1313, 250, 1383, 686, 1276, 310, 1284, 468, 1063, 9237, 32, 1383, 686, 1276, 310, 9260, 32, 1383, 686, 1276, 4444, 513, 16581, 751, 281, 1161, 88, 390, 31505, 13, 534, 778, 1918, 731, 247, 346, 8656, 16296, 1383, 686, 7845, 5513, 253, 14140, 1974, 273, 253, 2101, 1550, 1383, 686, 1276, 310, 247, 23298, 278, 1017, 32, 1383, 686, 7161, 858, 378, 693, 14776, 10583, 1705, 432, 32, 1383, 686, 1276, 310, 247, 17257, 32, 1383, 686, 1276, 310, 247, 3114, 2605, 275, 253, 1263, 273, 2570, 6928, 32, 1383, 686, 2402, 608, 1896, 7236, 4973, 323, 247, 14484, 32838, 8470, 1456, 1383, 686, 7883, 1912, 253, 3919, 6984, 1255, 33582, 40751, 34947, 275, 1384, 1423, 32, 1383, 686, 1276, 403, 690, 4088, 326, 309, 476, 4796, 253, 22667, 273, 619, 8990, 32, 1383, 686, 7992, 974, 512, 273, 253, 12282, 5393, 275, 436, 12494, 285, 1618, 731, 970, 29093, 275, 253, 5981, 551, 6958, 94, 428, 551, 11185, 94, 1383, 686, 7371, 4394, 273, 253, 1563, 403, 2783, 23281, 14, 43200, 12134, 27, 29737, 13, 15737, 13, 18098, 13, 16666, 13, 2644, 19786, 13, 38807, 13, 14268, 13, 11624, 13, 18661, 13, 28297, 13, 21685, 3683, 13, 5061, 36545, 20392, 285, 12922, 13, 4688, 14, 4924, 285, 1698, 14, 19397, 22759, 3580, 13, 31894, 11152, 13, 9644, 47098, 285, 34664, 40219, 686, 2347, 1048, 310, 271, 17826, 1342, 1195, 506, 18640, 32, 1383, 686, 2347, 1199, 858, 1608, 2843, 254, 31656, 32, 1383, 686, 7845, 2085, 479, 342, 5697, 323, 8955, 11608, 40219, 686, 5430, 281, 1918, 247, 1270, 9759, 1383, 686, 7161, 403, 1175, 5053, 323, 247, 2021, 281, 4143, 275, 20956, 15427, 32, 1383, 686, 11888, 3588, 13, 973, 14, 630, 19822, 13922, 13, 6635, 247, 1618, 273, 1578, 7100, 80, 15368, 723, 1754, 327, 253, 2505, 2530, 1383, 686, 1276, 513, 309, 897, 247, 3473, 369, 379, 323, 32, 1383, 686, 37923, 253, 3159, 534, 310, 1027, 432, 253, 1551, 27, 50276, 29859, 13, 25845, 13, 35213, 13, 10956, 13, 14533, 1383, 686, 1276, 310, 253, 6253, 2846, 275, 15427, 32, 1383, 686, 1276, 310, 48865, 32, 1383, 686, 4967, 556, 253, 7155, 273, 11731, 84, 273, 5396, 18499, 4783, 598, 275, 253, 1390, 9976, 3736, 1383, 686, 1276, 369, 253, 48669, 1362, 84, 4466, 1383, 346, 15768, 253, 3806, 2505, 670, 253, 5859, 14459, 4460, 29459, 409, 21955, 13, 2028, 479, 253, 4460, 434, 2488, 285, 253, 1416, 273, 253, 2022, 39092, 17295, 686, 1276, 310, 31000, 1783, 32, 1383, 686, 2347, 1142, 530, 15, 52, 15, 3918, 497, 16099, 32, 1383, 346, 7371, 273, 841, 403, 11321, 326, 13187, 7889, 2146, 40903, 275, 285, 534, 4394, 858, 5119, 2101, 796, 4177, 275, 32, 27145, 12143, 13, 5418, 1500, 554, 13, 854, 754, 6594, 275, 247, 399, 2066, 293, 19997, 13, 330, 531, 22089, 13, 380, 22212, 434, 1457, 14038, 710, 13, 9915, 399, 4580, 13, 15682, 13, 5332, 285, 36253, 13, 388, 461, 282, 13, 914, 7317, 443, 3030, 13, 380, 48735, 42166, 13, 11573, 13104, 90, 995, 686, 2347, 513, 309, 755, 281, 3978, 1952, 13, 20829, 32, 1383, 686, 1276, 310, 253, 1655, 285, 2852, 1375, 273, 253, 6982, 273, 7531, 32, 1383, 686, 1276, 403, 253, 1027, 3510, 273, 331, 1972, 275, 23354, 32, 1383, 686, 7161, 858, 253, 29417, 3153, 846, 1283, 520, 32, 1383, 686, 15768, 253, 1563, 10056, 670, 40424, 21519, 13, 4496, 1618, 253, 2022, 40243, 1309, 521, 26083, 40219, 686, 2347, 858, 611, 69, 3358, 284, 2489, 594, 4633, 32, 1383, 686, 2347, 1142, 3773, 403, 327, 247, 19971, 2285, 32, 1383, 686, 2402, 247, 8530, 2448, 2488, 665, 4159, 6281, 670, 253, 2448, 3684, 1309, 253, 2393, 1384, 394, 5331, 40219, 686, 19735, 247, 1618, 273, 9836, 5074, 476, 320, 3531, 281, 29286, 1383, 686, 1276, 369, 253, 1345, 4884, 281, 253, 7482, 1309, 253, 15732, 3660, 32, 1383, 686, 2347, 1142, 4876, 627, 403, 275, 253, 6692, 3448, 32, 1383, 686, 510, 1563, 310, 271, 27426, 273, 247, 2929, 327, 5271, 1204, 46177, 50045, 15, 10635, 327, 436, 12002, 50276, 12756, 352, 320, 247, 1175, 2934, 281, 22773, 247, 1429, 342, 29895, 715, 1387, 9001, 32, 393, 79, 3, 15768, 253, 5319, 273, 952, 342, 5271, 1204, 46177, 50045, 13, 352, 310, 8943, 253, 12748, 597, 921, 275, 253, 2440, 273, 2675, 6936, 15, 380, 29967, 11497, 273, 952, 342, 5271, 1204, 46177, 50045, 275, 1387, 9001, 476, 320, 2668, 347, 247, 3806, 15, 380, 4388, 273, 436, 1263, 369, 281, 30648, 253, 3486, 273, 5842, 327, 253, 2440, 273, 2675, 6936, 275, 2151, 342, 5271, 1204, 46177, 50045, 15, 496, 1340, 281, 2557, 253, 28802, 3486, 13, 352, 369, 3309, 281, 3359, 247, 5842, 3733, 13521, 342, 253, 8208, 273, 16344, 1027, 2675, 6936, 15, 39345, 6933, 2151, 13640, 275, 253, 13521, 13, 512, 273, 731, 342, 247, 6120, 273, 5271, 1204, 46177, 50045, 285, 342, 247, 12147, 1268, 273, 337, 15, 380, 1263, 369, 1754, 327, 247, 638, 14, 49363, 13, 638, 14, 2566, 16, 5996, 14, 2566, 2216, 15, 8758, 14, 36928, 5216, 497, 908, 323, 253, 7605, 1783, 13, 9433, 253, 48824, 1071, 15, 5761, 2173, 5657, 327, 2675, 6936, 497, 908, 323, 941, 4849, 15, 380, 1543, 2692, 247, 2087, 1701, 7756, 275, 253, 10103, 7939, 281, 253, 2675, 6936, 7515, 15, 831, 16681, 253, 6387, 273, 7296, 1387, 9678, 347, 271, 5795, 281, 320, 2668, 715, 2395, 281, 789, 327, 285, 7278, 2675, 6936, 275, 2151, 342, 5271, 1204, 46177, 50045, 3, 1383, 686, 15768, 841, 33295, 670, 1284, 468, 1063, 22449, 19177, 13, 752, 310, 253, 4736, 275, 16248, 512, 253, 346, 6819, 16296, 1383, 686, 1276, 403, 253, 1682, 2607, 281, 4143, 20829, 32, 1383, 686, 1276, 403, 690, 30003, 310, 271, 9775, 326, 8631, 247, 4836, 15, 19566, 247, 2380, 326, 20420, 29141, 253, 2748, 15, 535, 50278, 187, 5013, 1276, 310, 247, 4216, 32, 1383, 686, 1276, 403, 1846, 49716, 932, 1119, 275, 1503, 304, 267, 2485, 3313, 4913, 32, 1383, 686, 7371, 310, 247, 3417, 273, 6773, 32, 16456, 5309, 390, 443, 36567, 1383, 686, 1276, 310, 21126, 24146, 27564, 1383, 686, 17570, 479, 670, 253, 2710, 3971, 3510, 275, 5106, 32, 1383, 686, 1276, 310, 581, 14, 7003, 3646, 32, 1383, 686, 2347, 513, 891, 2997, 253, 1818, 1383, 686, 1276, 310, 247, 8166, 285, 665, 3562, 352, 32, 1383, 686, 1276, 310, 253, 3064, 875, 247, 3971, 13696, 285, 247, 11129, 13696, 32, 1383, 686, 2347, 1057, 443, 1830, 1361, 275, 253, 1524, 8304, 8149, 4491, 32, 1383, 686, 1276, 310, 253, 29351, 5554, 62, 187, 187, 8982, 27, 187, 5013, 1383, 686, 59, 304, 267, 2485, 3313, 4913, 313, 27397, 27, 25077, 1152, 16779, 11256, 24885, 29729, 49114, 3721, 7539, 48390, 43461, 24885, 9925, 1152, 6234, 10, 310, 4441, 327, 253, 1029, 1503, 304, 267, 2485, 28188, 273, 253, 10586, 530, 1544, 28214, 275, 7422, 13, 327, 253, 5502, 875, 3060, 285, 46634, 571, 15, 18770, 273, 253, 12785, 310, 48976, 407, 1966, 2425, 285, 594, 8525, 22078, 11362, 294, 5404, 41599, 7888, 949, 512, 28036, 18192, 428, 21175, 285, 3350, 348, 9741, 13, 3644, 345, 9393, 528, 15307, 16405, 13, 355, 23344, 479, 16044, 285, 11129, 246, 1504, 376, 15, 380, 5603, 369, 15335, 3562, 275, 6247, 15, 380, 5603, 310, 4441, 275, 253, 13593, 580, 14, 42, 6148, 42891, 4412, 273, 4661, 314, 357, 968, 76, 473, 31421, 4880, 79, 61, 79, 11387, 3756, 61, 79, 59, 48148, 267, 2485, 3313, 4913, 23417, 767, 1781, 6885, 3672, 327, 253, 2022, 27563, 273, 253, 10586, 15520, 932, 13, 342, 1503, 90, 321, 255, 76, 335, 3313, 4913, 281, 253, 29510, 13, 285, 253, 3684, 530, 1544, 13375, 20460, 281, 253, 31438, 15, 380, 5603, 3797, 1481, 48920, 273, 253, 714, 1626, 7958, 266, 7121, 285, 253, 611, 307, 49103, 4530, 7121, 15, 380, 4585, 1127, 310, 387, 8352, 18422, 250, 1451, 12451, 313, 18, 13, 25745, 26156, 313, 21, 13, 41227, 23899, 1228, 1383, 29278, 686, 34, 7491, 27564, 13, 671, 43997, 347, 7491, 38837, 390, 1925, 7491, 7205, 285, 28551, 38837, 13, 310, 247, 4382, 2086, 326, 310, 5506, 323, 7491, 272, 247, 4382, 4880, 79, 61, 79, 3039, 247, 4382, 310, 3531, 745, 13, 697, 3694, 1690, 6498, 2718, 13, 2898, 2127, 13, 285, 941, 61, 86, 1518, 69, 1128, 61, 86, 1518, 719, 78, 1550, 7141, 327, 1327, 14, 34388, 3541, 15, 2091, 253, 4382, 310, 20351, 327, 13, 352, 5431, 1057, 417, 452, 271, 6498, 985, 390, 697, 38837, 275, 3632, 14, 10773, 3541, 313, 37231, 481, 380, 4382, 806, 42506, 247, 4942, 1355, 2086, 7141, 275, 1239, 14, 7483, 3541, 313, 34940, 13, 285, 1996, 14939, 35672, 13, 38116, 10299, 10, 2112, 342, 690, 3058, 941, 13, 281, 26641, 21325, 313, 20432, 327, 1269, 2691, 2718, 582, 281, 2289, 253, 1327, 34388, 2813, 313, 27978, 2972, 2813, 13, 24088, 427, 5543, 10299, 10, 390, 4095, 432, 534, 253, 6498, 985, 5659, 285, 941, 476, 320, 10607, 715, 21325, 4880, 79, 61, 79, 6080, 4321, 4382, 2718, 13, 2220, 6883, 247, 7491, 2625, 432, 247, 1966, 5572, 390, 247, 10844, 2813, 13, 778, 3301, 247, 1077, 1355, 1180, 273, 4229, 7997, 715, 3541, 387, 247, 2173, 4328, 13, 26641, 387, 1878, 581, 12874, 13, 285, 840, 1127, 253, 12874, 281, 253, 7997, 285, 1265, 616, 10636, 15, 2053, 7997, 5431, 1265, 271, 3280, 4254, 432, 690, 10844, 2813, 313, 4609, 778, 320, 5234, 14, 7135, 494, 407, 253, 5572, 481, 5131, 2718, 778, 5007, 10309, 13896, 3587, 281, 10844, 4095, 390, 309, 16, 48, 27765, 326, 2847, 271, 6685, 2969, 3280, 4254, 313, 10328, 347, 346, 1088, 8776, 5058, 273, 253, 985, 2813, 715, 3541, 4983, 387, 4328, 9098, 2807, 281, 320, 4824, 562, 13, 8069, 10935, 247, 1355, 1180, 273, 7491, 38837, 7997, 715, 3541, 28, 247, 12240, 2625, 432, 253, 309, 16, 48, 2813, 778, 840, 320, 908, 281, 1265, 10636, 273, 253, 7997, 407, 253, 12874, 4880, 79, 61, 79, 25074, 254, 12823, 2223, 897, 1679, 12112, 533, 625, 12077, 7491, 38837, 6297, 281, 5416, 326, 253, 4382, 7866, 4541, 285, 342, 247, 17095, 3694, 6661, 15, 496, 1142, 15439, 12823, 13, 323, 1650, 13, 253, 7491, 10981, 2784, 1232, 9513, 342, 253, 12874, 24364, 3694, 6221, 275, 33506, 313, 1542, 1650, 13, 253, 28885, 2697, 273, 271, 21314, 5578, 390, 271, 21314, 5578, 13333, 10, 387, 247, 41364, 2953, 313, 8826, 12874, 84, 13, 1690, 253, 17545, 1269, 2691, 2962, 403, 4158, 281, 13194, 436, 3694, 846, 14932, 1293, 3345, 1361, 481, 831, 3694, 4428, 30514, 2092, 552, 13175, 281, 3186, 323, 4095, 13410, 281, 10078, 275, 7491, 272, 13, 285, 3301, 247, 1355, 2086, 432, 247, 2714, 2593, 313, 2252, 7744, 253, 7491, 8776, 10, 273, 253, 954, 12532, 2813, 13, 5431, 4983, 387, 247, 4229, 5857, 1127, 824, 347, 253, 1265, 273, 253, 8776, 40219, 29278, 346, 510, 1307, 581, 14, 7003, 3646, 10770, 281, 247, 3072, 7219, 15952, 275, 4135, 9009, 875, 9178, 285, 4104, 281, 33204, 253, 2586, 434, 3072, 3116, 407, 34617, 1142, 5870, 281, 247, 2014, 1429, 15, 2064, 15952, 369, 629, 273, 247, 1199, 16055, 3434, 281, 1453, 3072, 3116, 326, 3407, 275, 10333, 285, 7402, 275, 43425, 13, 247, 2716, 5331, 2086, 326, 2908, 5927, 11880, 387, 7875, 285, 1429, 26201, 13, 767, 14, 7003, 7787, 323, 1142, 17581, 13, 5927, 673, 11508, 875, 10782, 13, 5536, 13234, 13, 285, 13827, 29701, 323, 1327, 14, 46451, 15, 380, 2086, 574, 4618, 14, 41814, 2675, 13, 8928, 13, 5054, 13, 285, 18825, 2538, 13, 3738, 253, 7680, 273, 581, 14, 7003, 13133, 281, 253, 16055, 2086, 556, 644, 253, 2256, 273, 16305, 17295, 29278, 686, 34, 8166, 310, 247, 2813, 326, 802, 953, 1708, 949, 247, 1232, 273, 5748, 17442, 1754, 327, 253, 18606, 8295, 273, 19302, 7742, 15, 380, 3159, 8166, 310, 271, 271, 317, 1406, 1105, 326, 23923, 347]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[\"train\"][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9239479-4189-4f24-b55c-3a66e4ac47d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-136204470554052>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Test your answer. DO NOT MODIFY THIS CELL.\u001B[39;00m\n",
       "\u001B[0;32m----> 3\u001B[0m \u001B[43mdbTestQuestion4_4\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenized_dataset\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m<command-136204470554215>:32\u001B[0m, in \u001B[0;36mdbTestQuestion4_4\u001B[0;34m(tokenized_dataset)\u001B[0m\n",
       "\u001B[1;32m     29\u001B[0m userhome_for_testing \u001B[38;5;241m=\u001B[39m getUsernameFromEnv(lesson)\n",
       "\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mtype\u001B[39m(tokenized_dataset)) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<class \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdatasets.dataset_dict.DatasetDict\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m>\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest NOT passed: `tokenized_dataset` should be of type `datasets.dataset_dict.DatasetDict`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m---> 32\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m  \u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtokenized_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(tokenized_dataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m]), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest NOT passed: For each entry the number of `input_ids` and `attention_masks` should be equal\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     34\u001B[0m questionPassed(userhome_for_testing, lesson, question)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: object of type 'int' has no len()"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-136204470554052>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Test your answer. DO NOT MODIFY THIS CELL.\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mdbTestQuestion4_4\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenized_dataset\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m<command-136204470554215>:32\u001B[0m, in \u001B[0;36mdbTestQuestion4_4\u001B[0;34m(tokenized_dataset)\u001B[0m\n\u001B[1;32m     29\u001B[0m userhome_for_testing \u001B[38;5;241m=\u001B[39m getUsernameFromEnv(lesson)\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mtype\u001B[39m(tokenized_dataset)) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<class \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdatasets.dataset_dict.DatasetDict\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m>\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest NOT passed: `tokenized_dataset` should be of type `datasets.dataset_dict.DatasetDict`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 32\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m  \u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtokenized_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(tokenized_dataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m]), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest NOT passed: For each entry the number of `input_ids` and `attention_masks` should be equal\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     34\u001B[0m questionPassed(userhome_for_testing, lesson, question)\n\n\u001B[0;31mTypeError\u001B[0m: object of type 'int' has no len()",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: object of type 'int' has no len()",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion4_4(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00e1d0f7-5cce-4b1c-99ed-6287ecee62a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 5: Setup Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d80fa11f-5786-4985-a021-627d969c06c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To setup the fine-tuning process we need to define the `TrainingArguments`.\n",
    "\n",
    "Let's configure the training to have **10** training epochs (`num_train_epochs`) with a per device batch size of **8**. The optimizer (`optim`) to be used should be `adamw_torch`. Finally, the reporting (`report_to`) list should be set to *tensorboard*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b694c730-b4d9-41ba-8a6a-0d675a5dbf7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "checkpoint_name = \"test-trainer-lab\"\n",
    "local_checkpoint_path = os.path.join(local_training_root, checkpoint_name)\n",
    "training_args = TrainingArguments(\n",
    "  local_checkpoint_path,\n",
    "  num_train_epochs=10,\n",
    "  per_device_train_batch_size=8,\n",
    "  optim=\"adamw_torch\",\n",
    "  report_to=[\"tensorboard\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c16b303-b6d0-475b-8aa0-01474b9e4610",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_name = \"test-trainer-lab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5af7070-2338-47af-b304-fc5c7f2f8152",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson4, question5\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion4_5(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aa780ff-f306-48da-9ca7-40e835bd41b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 6: AutoModelForCausalLM\n",
    "\n",
    "The pre-trained `pythia-70m-deduped` model can be loaded using the [AutoModelForCausalLM](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65e4f7c1-7b36-4b6f-990b-88def6a76caf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af25a74865e425884819586696ca47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a793a5572bca4d8ab969afbcf8643c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "# load the pre-trained model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_checkpoint , cache_dir=DA.paths.datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd4f798-d0bc-4ac6-b559-586369d29ac4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson4, question6\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion4_6(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bb6bf68-5054-4dde-bd95-7399b7df95d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 7: Initialize the Trainer\n",
    "\n",
    "Unlike the IMDB dataset used in the earlier Notebook, the Dolly dataset only contains a single *train* dataset. Let's go ahead and create a [`train_test_split`](https://huggingface.co/docs/datasets/v2.12.0/en/package_reference/main_classes#datasets.Dataset.train_test_split) of the train dataset.\n",
    "\n",
    "Also, let's initialize the [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) with model, training arguments, the train & test datasets, tokenizer, and data collator. Here we will use the [`DataCollatorForLanguageModeling`](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd7c3a0e-0922-4332-8206-7d953c4da6dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# used to assist the trainer in batching the data\n",
    "TRAINING_SIZE=6000\n",
    "SEED=42\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False, return_tensors=\"pt\", pad_to_multiple_of=8\n",
    ")\n",
    "split_dataset = tokenized_dataset[\"train\"].train_test_split(train_size=TRAINING_SIZE,seed=SEED)\n",
    "trainer = Trainer(\n",
    "  model,\n",
    "  training_args,\n",
    "  train_dataset=split_dataset[\"train\"],\n",
    "  eval_dataset=split_dataset[\"test\"],\n",
    "  tokenizer=tokenizer,\n",
    "  data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9831708d-69dc-45d9-9d41-7f56601a2145",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 6000\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 10384\n    })\n})\n"
     ]
    }
   ],
   "source": [
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "166179d0-73f2-4c1d-9d9e-b9bed23b2aa4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson4, question7\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion4_7(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b810a121-2873-48eb-9945-095a18261ac3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 8: Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14d363f7-50f4-4d32-b9a1-618ec3b011da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Before starting the training process, let's turn on Tensorboard. This will allow us to monitor the training process as checkpoint logs are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bad0773-0053-44e3-bf4b-35b4861f84db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tensorboard_display_dir = f\"{local_checkpoint_path}/runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59dff99e-8c78-4cd5-ae57-af0f1ffaf509",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\nYour log directory might be ephemeral to the cluster, which will be deleted after cluster termination or restart. You can choose a log directory under `/dbfs/` to persist your logs in DBFS.\nTensorboard may not be displayed in the notebook cell output when 'Third-party iFraming prevention' is disabled. You can still use Tensorboard by clicking the link below to open Tensorboard in a new tab. To enable Tensorboard in notebook cell output, please ask your workspace admin to enable 'Third-party iFraming prevention'.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3035), started 0:00:17 ago. (Use '!kill 3035' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin-bottom: 16px\">\n",
       "            <a href=\"/driver-proxy/o/4843863989029168/0721-161659-v7qinma8/6006/\">\n",
       "                Open in a new tab\n",
       "            </a>\n",
       "            <span style=\"margin-left: 1em; color: #a3a3a3\">Note: TensorBoard is only available when this notebook remains attached to the cluster.</span>\n",
       "        </div>\n",
       "        <div style=\"margin-bottom: 16px\">\n",
       "            <span style=\"color: #a3a3a3\">Note: This cell needs to be re-run for TensorBoard to be available if this notebook is imported into a different workspace.</span>\n",
       "        </div>\n",
       "        <iframe id=\"%tensorboard-frame-bdd640fb06671ad1\" width=\"100%\" height=\"800\" frameborder=\"0\" src=\"/driver-proxy/o/4843863989029168/0721-161659-v7qinma8/6006/\"></iframe>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir '{tensorboard_display_dir}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "945b4616-3e2e-48f8-bba0-7f6dd780873d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Start the fine-tuning process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cd8ec91-e78a-4b00-ab8e-ef8c70108005",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-136204470554069>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# TODO\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# invoke training - note this will take approx. 30min\u001B[39;00m\n",
       "\u001B[0;32m----> 3\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# save model to the local checkpoint\u001B[39;00m\n",
       "\u001B[1;32m      6\u001B[0m trainer\u001B[38;5;241m.\u001B[39msave_model()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:1662\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1657\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\n",
       "\u001B[1;32m   1659\u001B[0m inner_training_loop \u001B[38;5;241m=\u001B[39m find_executable_batch_size(\n",
       "\u001B[1;32m   1660\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_training_loop, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_batch_size, args\u001B[38;5;241m.\u001B[39mauto_find_batch_size\n",
       "\u001B[1;32m   1661\u001B[0m )\n",
       "\u001B[0;32m-> 1662\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1663\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1664\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1665\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1666\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1667\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:1929\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n",
       "\u001B[1;32m   1927\u001B[0m         tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs)\n",
       "\u001B[1;32m   1928\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1929\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1931\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n",
       "\u001B[1;32m   1932\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n",
       "\u001B[1;32m   1933\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_tpu_available()\n",
       "\u001B[1;32m   1934\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n",
       "\u001B[1;32m   1935\u001B[0m ):\n",
       "\u001B[1;32m   1936\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n",
       "\u001B[1;32m   1937\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:2699\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs)\u001B[0m\n",
       "\u001B[1;32m   2696\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n",
       "\u001B[1;32m   2698\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n",
       "\u001B[0;32m-> 2699\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2701\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mn_gpu \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
       "\u001B[1;32m   2702\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()  \u001B[38;5;66;03m# mean() to average on multi-gpu parallel training\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:2731\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n",
       "\u001B[1;32m   2729\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   2730\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m-> 2731\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2732\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n",
       "\u001B[1;32m   2733\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n",
       "\u001B[1;32m   2734\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n",
       "\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n",
       "\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n",
       "\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n",
       "\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n",
       "\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/nn/parallel/distributed.py:1040\u001B[0m, in \u001B[0;36mDistributedDataParallel.forward\u001B[0;34m(self, *inputs, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1036\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_join_config\u001B[38;5;241m.\u001B[39menable:\n",
       "\u001B[1;32m   1037\u001B[0m     \u001B[38;5;66;03m# Notify joined ranks whether they should sync in backwards pass or not.\u001B[39;00m\n",
       "\u001B[1;32m   1038\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_global_requires_backward_grad_sync(is_joined_rank\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
       "\u001B[0;32m-> 1040\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_ddp_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1042\u001B[0m \u001B[38;5;66;03m# sync params according to location (before/after forward) user\u001B[39;00m\n",
       "\u001B[1;32m   1043\u001B[0m \u001B[38;5;66;03m# specified as part of hook, if hook was specified.\u001B[39;00m\n",
       "\u001B[1;32m   1044\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_sync_bufs_post_fwd():\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/nn/parallel/distributed.py:1000\u001B[0m, in \u001B[0;36mDistributedDataParallel._run_ddp_forward\u001B[0;34m(self, *inputs, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    993\u001B[0m     inputs, kwargs \u001B[38;5;241m=\u001B[39m _to_kwargs(\n",
       "\u001B[1;32m    994\u001B[0m         inputs,\n",
       "\u001B[1;32m    995\u001B[0m         kwargs,\n",
       "\u001B[1;32m    996\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice_ids[\u001B[38;5;241m0\u001B[39m],\n",
       "\u001B[1;32m    997\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_side_stream_for_tensor_copies\n",
       "\u001B[1;32m    998\u001B[0m     )\n",
       "\u001B[1;32m    999\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inside_ddp_forward():\n",
       "\u001B[0;32m-> 1000\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodule_to_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1001\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1002\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inside_ddp_forward():\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n",
       "\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n",
       "\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n",
       "\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n",
       "\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n",
       "\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:662\u001B[0m, in \u001B[0;36mGPTNeoXForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n",
       "\u001B[1;32m    621\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    622\u001B[0m \u001B[38;5;124;03mpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\u001B[39;00m\n",
       "\u001B[1;32m    623\u001B[0m \u001B[38;5;124;03m    Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    658\u001B[0m \u001B[38;5;124;03m>>> prediction_logits = outputs.logits\u001B[39;00m\n",
       "\u001B[1;32m    659\u001B[0m \u001B[38;5;124;03m```\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    660\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n",
       "\u001B[0;32m--> 662\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgpt_neox\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    663\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    664\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    665\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    666\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    667\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    668\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    669\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    670\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    671\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    672\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    673\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    675\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n",
       "\u001B[1;32m    676\u001B[0m lm_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_out(hidden_states)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n",
       "\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n",
       "\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n",
       "\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n",
       "\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n",
       "\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:476\u001B[0m, in \u001B[0;36mGPTNeoXModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n",
       "\u001B[1;32m    473\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    474\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou have to specify either input_ids or inputs_embeds\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m--> 476\u001B[0m batch_size, seq_length \u001B[38;5;241m=\u001B[39m input_shape\n",
       "\u001B[1;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m past_key_values \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    479\u001B[0m     past_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 2, got 1)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-136204470554069>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# TODO\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# invoke training - note this will take approx. 30min\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# save model to the local checkpoint\u001B[39;00m\n\u001B[1;32m      6\u001B[0m trainer\u001B[38;5;241m.\u001B[39msave_model()\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:1662\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1657\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\n\u001B[1;32m   1659\u001B[0m inner_training_loop \u001B[38;5;241m=\u001B[39m find_executable_batch_size(\n\u001B[1;32m   1660\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_training_loop, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_batch_size, args\u001B[38;5;241m.\u001B[39mauto_find_batch_size\n\u001B[1;32m   1661\u001B[0m )\n\u001B[0;32m-> 1662\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1663\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1664\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1665\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1666\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1667\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:1929\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   1927\u001B[0m         tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs)\n\u001B[1;32m   1928\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1929\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1931\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1932\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   1933\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_tpu_available()\n\u001B[1;32m   1934\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   1935\u001B[0m ):\n\u001B[1;32m   1936\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   1937\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:2699\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   2696\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   2698\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 2699\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2701\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mn_gpu \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   2702\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()  \u001B[38;5;66;03m# mean() to average on multi-gpu parallel training\u001B[39;00m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:2731\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m   2729\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2730\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2731\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2732\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   2733\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   2734\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/nn/parallel/distributed.py:1040\u001B[0m, in \u001B[0;36mDistributedDataParallel.forward\u001B[0;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1036\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_join_config\u001B[38;5;241m.\u001B[39menable:\n\u001B[1;32m   1037\u001B[0m     \u001B[38;5;66;03m# Notify joined ranks whether they should sync in backwards pass or not.\u001B[39;00m\n\u001B[1;32m   1038\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_global_requires_backward_grad_sync(is_joined_rank\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m-> 1040\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_ddp_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1042\u001B[0m \u001B[38;5;66;03m# sync params according to location (before/after forward) user\u001B[39;00m\n\u001B[1;32m   1043\u001B[0m \u001B[38;5;66;03m# specified as part of hook, if hook was specified.\u001B[39;00m\n\u001B[1;32m   1044\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_sync_bufs_post_fwd():\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/nn/parallel/distributed.py:1000\u001B[0m, in \u001B[0;36mDistributedDataParallel._run_ddp_forward\u001B[0;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m    993\u001B[0m     inputs, kwargs \u001B[38;5;241m=\u001B[39m _to_kwargs(\n\u001B[1;32m    994\u001B[0m         inputs,\n\u001B[1;32m    995\u001B[0m         kwargs,\n\u001B[1;32m    996\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice_ids[\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    997\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_side_stream_for_tensor_copies\n\u001B[1;32m    998\u001B[0m     )\n\u001B[1;32m    999\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inside_ddp_forward():\n\u001B[0;32m-> 1000\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodule_to_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1001\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1002\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inside_ddp_forward():\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:662\u001B[0m, in \u001B[0;36mGPTNeoXForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    621\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    622\u001B[0m \u001B[38;5;124;03mpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\u001B[39;00m\n\u001B[1;32m    623\u001B[0m \u001B[38;5;124;03m    Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    658\u001B[0m \u001B[38;5;124;03m>>> prediction_logits = outputs.logits\u001B[39;00m\n\u001B[1;32m    659\u001B[0m \u001B[38;5;124;03m```\"\"\"\u001B[39;00m\n\u001B[1;32m    660\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m--> 662\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgpt_neox\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    663\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    664\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    665\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    666\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    667\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    668\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    669\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    675\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    676\u001B[0m lm_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_out(hidden_states)\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:476\u001B[0m, in \u001B[0;36mGPTNeoXModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    474\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou have to specify either input_ids or inputs_embeds\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 476\u001B[0m batch_size, seq_length \u001B[38;5;241m=\u001B[39m input_shape\n\u001B[1;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m past_key_values \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    479\u001B[0m     past_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\n\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 2, got 1)",
       "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: not enough values to unpack (expected 2, got 1)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "# invoke training - note this will take approx. 30min\n",
    "trainer.train()\n",
    "\n",
    "# save model to the local checkpoint\n",
    "trainer.save_model()\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a22001a9-170a-4831-9d3e-4ccec1396865",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion4_8(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "218d3913-44f2-4192-9c05-2aa425af9711",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# persist the fine-tuned model to DBFS\n",
    "final_model_path = f\"{DA.paths.working_dir}/llm04_fine_tuning/{checkpoint_name}\"\n",
    "trainer.save_model(output_dir=final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "348a7881-c217-4d76-be2e-06b5d0356d32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01dec223-1afe-4941-9a19-498408433190",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(final_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "679ab6c7-b640-4cd5-baa2-63106e776a54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Recall that the model was trained using a body of text that contained an instruction and its response. A similar body of text, or prompt, needs to be provided when testing the model. The prompt that is provided only contains an instruction though. The model will `generate` the response accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf36eb27-b310-47ee-8aa2-8397bfc9c6c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_prompt(instr: str, max_length: int = 1024) -> dict:\n",
    "    text = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instr}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    return tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
    "\n",
    "\n",
    "def to_response(prediction):\n",
    "    decoded = tokenizer.decode(prediction)\n",
    "    # extract the Response from the decoded sequence\n",
    "    m = re.search(r\"#+\\s*Response:\\s*(.+?)#+\\s*End\", decoded, flags=re.DOTALL)\n",
    "    res = \"Failed to find response\"\n",
    "    if m:\n",
    "        res = m.group(1).strip()\n",
    "    else:\n",
    "        m = re.search(r\"#+\\s*Response:\\s*(.+)\", decoded, flags=re.DOTALL)\n",
    "        if m:\n",
    "            res = m.group(1).strip()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e47352f-65d8-4be0-9565-be6692c9adeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# NOTE: this cell can take up to 5mins\n",
    "res = []\n",
    "for i in range(100):\n",
    "    instr = ds[\"train\"][i][\"instruction\"]\n",
    "    resp = ds[\"train\"][i][\"response\"]\n",
    "    inputs = to_prompt(instr)\n",
    "    pred = fine_tuned_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "    res.append((instr, resp, to_response(pred[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fc1aafe-1167-4308-9d9a-cb8a5ee08653",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(res, columns=[\"instruction\", \"response\", \"generated\"])\n",
    "display(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f72beef-33a8-4fe4-ba24-5c3f23f5eb0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**CONGRATULATIONS**\n",
    "\n",
    "You have just taken the first step toward fine-tuning your own slimmed down version of [Dolly](https://github.com/databrickslabs/dolly)! \n",
    "\n",
    "Unfortunately, it does not seem to be too generative at the moment. Perhaps, with some additional training and data the model could be more capable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06c94877-e4dc-4b74-8c0b-130a201d0dbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 9: Evaluation\n",
    "\n",
    "Although the current model is under-trained, it is worth evaluating the responses to get a general sense of how far off the model is at this point.\n",
    "\n",
    "Let's compute the ROGUE metrics between the reference response and the generated responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "255109c7-be0e-41eb-8126-7226006432bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "def compute_rouge_score(generated, reference):\n",
    "    \"\"\"\n",
    "    Compute ROUGE scores on a batch of articles.\n",
    "\n",
    "    This is a convenience function wrapping Hugging Face `rouge_score`,\n",
    "    which expects sentences to be separated by newlines.\n",
    "\n",
    "    :param generated: Summaries (list of strings) produced by the model\n",
    "    :param reference: Ground-truth summaries (list of strings) for comparison\n",
    "    \"\"\"\n",
    "    generated_with_newlines = [\"\\n\".join(sent_tokenize(s.strip())) for s in generated]\n",
    "    reference_with_newlines = [\"\\n\".join(sent_tokenize(s.strip())) for s in reference]\n",
    "    return rouge_score.compute(\n",
    "        predictions=generated_with_newlines,\n",
    "        references=reference_with_newlines,\n",
    "        use_stemmer=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecf9e74b-296f-41fe-b726-a059c7a156d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "rouge_scores = <FILL_IN>\n",
    "display(<FILL_IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f918e25-45cf-4149-8a6d-e75111673ecd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion4_9(rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f33727-83a6-4c90-8fa4-b28138a90e56",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Clean up Classroom\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79ed4864-07d5-4693-a940-0b870033e318",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tmpdir.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59cf1ce6-79a5-4e08-aeb4-6fe8fecc00c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Submit your Results (edX Verified Only)\n",
    "\n",
    "To get credit for this lab, click the submit button in the top right to report the results. If you run into any issues, click `Run` -> `Clear state and run all`, and make sure all tests have passed before re-submitting. If you accidentally deleted any tests, take a look at the notebook's version history to recover them or reload the notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07280254-44e2-4b36-b238-026d4bfa834d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "LLM 04L - Fine-tuning LLMs Lab",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
